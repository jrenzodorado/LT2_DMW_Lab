{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510f894c-02f9-4cb8-8b77-0d3dd52936f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd33b392-5d37-45bc-93eb-d701aeb080b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_parent(tag, pfr_list=True):\n",
    "    \"\"\"\n",
    "    Retrieves the parent tag of a given HTML tag.\n",
    "    It immediately returns the tag 'i' if found in the parent-finding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : bs4.element.Tag\n",
    "        The HTML tag whose parent is to be obtained.\n",
    "    pfr_list : bool, optional\n",
    "        If `True`, iterates through the primary tags (p, li, th, td) to find \n",
    "        the parent. If `False`, only returns the immediate parent.\n",
    "        Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parent : bs4.element.Tag or None\n",
    "        The parent tag of the input tag. Returns None if no parent is found.\n",
    "    \"\"\"\n",
    "\n",
    "    parent = tag.parent\n",
    "    if parent and parent.name == 'i':  # italics for exclusion later\n",
    "        return parent\n",
    "    parents = ['p', 'li', 'th', 'td']\n",
    "    if pfr_list:  # if not usual container, just use the first parent\n",
    "        while parent and parent.name not in parents:  # iterate until tag\n",
    "            parent = parent.parent\n",
    "            if parent and parent.name == 'i':  # italics for exclusion later\n",
    "                return parent\n",
    "    return parent\n",
    "\n",
    "\n",
    "def enclosed(tag, pfr_list=True):\n",
    "    \"\"\"\n",
    "    Checks if an HTML tag is enclosed within parentheses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : bs4.element.Tag\n",
    "        The HTML tag to be checked.\n",
    "    pfr_list : bool, optional\n",
    "        If `True`, considers the tag's immediate parent and its ancestors. If \n",
    "        `False`, only considers the immediate parent. Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the tag is enclosed within parentheses, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    parent = get_parent(tag, pfr_list)\n",
    "    if parent is None:\n",
    "        return False  # not enclosed if no parent\n",
    "    if parent.name == 'i':  # enclosed in italic tag\n",
    "        return True\n",
    "    parent_text = str(parent)  # includes the html tags\n",
    "    tag_text = str(tag)  # includes the html tags\n",
    "    idx = parent_text.index(tag_text)\n",
    "    l_cnt = parent_text[:idx].count('(')\n",
    "    r_cnt = parent_text[:idx].count(')')\n",
    "    if l_cnt == 0 or l_cnt - r_cnt != 1:  # left ( not found, or no open left\n",
    "        return False\n",
    "    l_cnt = parent_text[idx+len(tag_text):].count('(')\n",
    "    r_cnt = parent_text[idx+len(tag_text):].count(')')\n",
    "    # print('right >> ' + parent_text[idx+len(tag_text):])\n",
    "    if r_cnt == 0 or r_cnt - l_cnt != 1:  # right ( not found, or no open right\n",
    "        return False\n",
    "    return True  # enclosed\n",
    "\n",
    "\n",
    "def online(alink, url):\n",
    "    \"\"\"\n",
    "    Checks if an anchor link is online and accessible.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alink : bs4.element.Tag\n",
    "        The anchor link tag to be checked.\n",
    "    url : str\n",
    "        The base URL to which the anchor link is appended.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the anchor link is accessible, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(urljoin(url, alink['href']))\n",
    "    except Exception:\n",
    "        return False\n",
    "    if response.status_code == 200:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def wiki(alink, stop_words):\n",
    "    \"\"\"\n",
    "    Checks if an anchor link is a valid Wikipedia link.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alink : bs4.element.Tag\n",
    "        The anchor link tag to be checked.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the anchor link is a valid Wikipedia link, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    link = alink['href']\n",
    "    netloc = urlparse(link).netloc\n",
    "    if netloc == '' or 'wikipedia.org' in netloc:\n",
    "        return not any([word for word in stop_words if word in unquote(link)])\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_link(body, stop_words, url):\n",
    "    \"\"\"\n",
    "    Retrieves the first valid link within the primary tags (p, li, th, td)\n",
    "    of the main body.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body : bs4.element.Tag\n",
    "        The main content of the web page.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "    url : str\n",
    "        The base URL of the web page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link : str\n",
    "        The URL of the first valid link found.\n",
    "    \"\"\"\n",
    "\n",
    "    link = ''\n",
    "    a_str = ('a[href]:not([href^=\"#cite_note\"]):not([href^=\"#\"])'\n",
    "             ':not(.mw-disambig)')\n",
    "    p_str = ['p', 'li', 'th', 'td']\n",
    "    alinks = body.select(', '.join([f'{p} {a_str} ' for p in p_str]))\n",
    "    for alink in alinks:\n",
    "        if (wiki(alink, stop_words) and online(alink, url)\n",
    "                and not enclosed(alink)):\n",
    "            link = alink['href']\n",
    "            break\n",
    "    return link\n",
    "\n",
    "\n",
    "def get_other_link(body, extags, stop_words, url):\n",
    "    \"\"\"\n",
    "    Retrieves a valid link from the main body if no valid links are found\n",
    "    in get_link().\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body : bs4.element.Tag\n",
    "        The main content of the web page.\n",
    "    extags : list\n",
    "        List of extracted tags from the main content to be added back.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "    url : str\n",
    "        The base URL of the web page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link : str\n",
    "        The URL of another valid link found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    IndexError\n",
    "        If no valid links are found.\n",
    "    \"\"\"\n",
    "\n",
    "    link = ''\n",
    "    a_str = ('a[href]:not([href^=\"#cite_note\"]):not([href^=\"#\"]):'\n",
    "             'not(.mw-disambig)')\n",
    "    for extag in extags:  # add back extracted tags for full search\n",
    "        body.append(extag)\n",
    "    alinks = body.select(f'{a_str}')\n",
    "    for alink in alinks:\n",
    "        if (wiki(alink, stop_words) and online(alink, url)\n",
    "                and not enclosed(alink, False)):\n",
    "            link = alink['href']\n",
    "            break\n",
    "    if link == '':  # no valid links in the body\n",
    "        raise IndexError\n",
    "    return link\n",
    "\n",
    "def save_linkhist(linkhist, dir_, a, iter, fin=False):\n",
    "    \"\"\"\n",
    "    Saves the Link:DoS history to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    linkhist : dict\n",
    "        Dictionary containing link:DoS mappings.\n",
    "    dir_ : str\n",
    "        Directory where the CSV file will be saved.\n",
    "    a : int\n",
    "        Starting index used in the filename for identification.\n",
    "    iter : int\n",
    "        Iteration number used in the filename for identification.\n",
    "    fin : bool, optional\n",
    "        If `True`, indicates the final save. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    dflhist = pd.DataFrame(list(linkhist.items()), columns=['Link', 'DoS'])\n",
    "    buname = f'_bu{a:03d}.{iter:03d}' if not fin else ''\n",
    "    dflhist.to_csv(f'{dir_}linkhist{buname}.csv', index=False)\n",
    "    print(f'Link:DoS {\"backup \" if not fin else \"\"}history saved to {dir_}linkhist{buname}.csv')\n",
    "    return None\n",
    "\n",
    "def save_conso(data, dir_, a, iter, fin=False):\n",
    "    \"\"\"\n",
    "    Saves consolidated crawl results to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        List containing crawl data entries.\n",
    "    dir_ : str\n",
    "        Directory where the CSV file will be saved.\n",
    "    a : int\n",
    "        Starting index used in the filename for identification.\n",
    "    iter : int\n",
    "        Iteration number used in the filename for identification.\n",
    "    fin : bool, optional\n",
    "        If `True`, indicates the final save. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or None\n",
    "        Returns a DataFrame if `fin` is True; otherwise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Start', 'DoS', 'Path', 'Msg'])\n",
    "    buname = '_bu' if not fin else ''\n",
    "    df.to_csv(f'{dir_}conso{buname}{a:03d}.{iter:03d}.csv', index=False)\n",
    "    print(f'Consolidated {\"backup \" if not fin else \"\"}results saved to '\n",
    "          f'{dir_}conso{buname}{a:03d}.{iter:03d}.csv')\n",
    "    return df if fin else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afe29b8-3d40-4e8e-95a9-2b7455d29d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def web_crawler(a, dir_, url=None, lang='en', linkhist={}):\n",
    "    \"\"\"\n",
    "    This function starts from a random Wikipedia page and follows the first\n",
    "    page link until it reaches the Philosophy page, a page with no links, or\n",
    "    loops back to a previously visited link.\n",
    "    This function accepts a link:DoS (linkhist) dictionary which shorts the\n",
    "    path to Philosopy, removing redundant crawls from previous runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : int\n",
    "        Index for the web crawl.\n",
    "    dir_ : str\n",
    "        Directory path where the output scrape CSV file will be saved.\n",
    "    url : str, optional\n",
    "        The starting URL for the web crawl. If not provided, a random Wikipedia\n",
    "        page URL will be chosen based on the specified language.\n",
    "    lang : str, optional\n",
    "        The language code for the Wikipedia pages. Defaults to 'en' (English).\n",
    "        Other options: 'es' (Spanish), 'fr' (French), 'de' (German).\n",
    "    linkhist : dict, optional\n",
    "        Dictionary to store link history - used for the short path, reducing\n",
    "        crawl iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    flinks : list\n",
    "        List of visited page URLs during the crawl.\n",
    "    dosl : list\n",
    "        List of degrees of separation (DoS) for each visited page.\n",
    "    msg : str\n",
    "        Message indicating the status of the crawl (e.g., 'OK - normal path' or\n",
    "        'NOK - no links found').\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function excludes certain types of links (e.g., language links,\n",
    "      disambiguation pages, external links) during the crawl.\n",
    "    - It prioritizes links within specific HTML tags and considers the main tex\n",
    "      of the Wikipedia page.\n",
    "    - The DoS represents the number of steps from the starting page to the\n",
    "      Philosophy page.\n",
    "    - It outputs the single scrape result into a CSV file.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    flinks, dosl, msg = web_crawler(1, 'output/', lang='en')\n",
    "    \"\"\"\n",
    "\n",
    "    if not bool(url):\n",
    "        url = f'https://{lang}.wikipedia.org/wiki/Special:Random'\n",
    "    philo = {\n",
    "        'en': 'Philosophy',\n",
    "        'de': 'Philosophie',\n",
    "        'es': 'Psicología',\n",
    "        'fr': 'Philosophie'\n",
    "    }\n",
    "    target = f'https://{lang}.wikipedia.org/wiki/{philo[lang]}'\n",
    "    stop_words = {\n",
    "        'en': ['Wikipedia:', 'Wikipedia talk:', 'User:', 'User_talk:', 'Talk:',\n",
    "               'Help:', 'Help talk:', 'Project:', 'Project talk:',\n",
    "               'Portal:', 'Portal talk:', 'Template:', 'Template talk:',\n",
    "               'File:', 'File talk:', 'Special:', 'index.php',\n",
    "               'Category:', 'Category talk:', 'Template_talk:', 'MOS:', 'Special:'],\n",
    "        'de': ['Wikipedia:', 'Wikipedia Diskussion:', 'Benutzer:', 'Benutzer Diskussion:',\n",
    "               'Diskussion:',\n",
    "               'Hilfe:', 'Hilfe Diskussion:', 'Projekt:',\n",
    "               'Projekt Diskussion:',\n",
    "               'Portal:', 'Portal Diskussion:', 'Vorlage:',\n",
    "               'Vorlage Diskussion:',\n",
    "               'Datei:', 'Datei Diskussion:', 'Spezial:', 'index.php',\n",
    "               'Kategorie:', 'Kategorie Diskussion:', 'Vorlage Diskussion:',\n",
    "               'MOS:', 'Special:'],\n",
    "        'es': ['Wikipedia:', 'Wikipedia discusión:', 'Usuario:', 'Usuario discusión:',\n",
    "               'Discusión:',\n",
    "               'Ayuda:', 'Ayuda discusión:', 'Wikiproyecto:', 'Wikiproyecto discusión:',\n",
    "               'Portal:', 'Portal discusión:', 'Plantilla:', 'Plantilla discusión:',\n",
    "               'Archivo:', 'Archivo discusión:', 'Especial:', 'index.php',\n",
    "               'Categoría:', 'Categoría discusión:', 'Plantilla discusión:', 'WP:',\n",
    "               'Special:'],\n",
    "        'fr': ['Wikipédia:', 'Discussion Wikipédia:', 'Utilisateur:',\n",
    "               'Discussion utilisateur:', 'Discussion:', 'Aide:', 'Discussion aide:',\n",
    "               'Projet:', 'Discussion Projet:', 'Portail:', 'Discussion Portail:',\n",
    "               'Modèle:', 'Discussion modèle:', 'Fichier:', 'Discussion fichier:',\n",
    "               'Spécial:', 'index.php', 'Catégorie:', 'Discussion catégorie:',\n",
    "               'Discussion modèle:', 'MOS:', 'Spécial:']\n",
    "\n",
    "    }\n",
    "    s = requests.Session()\n",
    "    response = s.get(url)\n",
    "    link = urlparse(response.url).path\n",
    "    flink = urljoin(url, link)\n",
    "    flinks = []\n",
    "    flinks.append(flink)\n",
    "    print(link)\n",
    "    ldos = 0\n",
    "    dos = -1\n",
    "    msg = ''\n",
    "    while True:\n",
    "        response = s.get(flink)\n",
    "        soup = bs4.BeautifulSoup(response.text)\n",
    "        body = soup.find('div', id='bodyContent')\n",
    "        for table in body.find_all('table', class_=['infobox', 'sidebar',\n",
    "                                                    'metadata']):\n",
    "            table.extract()\n",
    "        for div in body.find_all('div', class_=['infobox', 'sidebar',\n",
    "                                                'metadata']):\n",
    "            div.extract()\n",
    "        for div in body.find_all('div', role='note'):\n",
    "            div.extract()\n",
    "        extags = []\n",
    "        for extag in body.find_all('div', class_='thumbcaption'):\n",
    "            extags.append(extag.extract())\n",
    "        for extag in body.find_all('figcaption'):\n",
    "            extags.append(extag.extract())\n",
    "        for extag in body.find_all('table', class_=['infobox',\n",
    "                                                    'standings-box']):\n",
    "            extags.append(extag.extract())\n",
    "        try:\n",
    "            link = get_link(body, stop_words[lang], url)\n",
    "            if link == '':  # no valid link within p, li, th, or td tags\n",
    "                link = get_other_link(body, extags, stop_words[lang], url)\n",
    "            flink = urljoin(url, link)\n",
    "        except IndexError:\n",
    "            print('No valid links found in current page.')\n",
    "            print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "            msg = f'NOK - no links at {link} (last idx: {len(flinks)-1})'\n",
    "            break\n",
    "        # link = link.split('wiki/')[-1]\n",
    "        if unquote(flink) == target:\n",
    "            flinks.append(flink)\n",
    "            print(link)\n",
    "            print(f'Successfully reached {philo[lang]} w/ DoS '\n",
    "                  f'= {len(flinks)-1}.')\n",
    "            msg = 'OK - normal path.'\n",
    "            break\n",
    "        elif flink in flinks:\n",
    "            print(f'Looped back to {link}.')\n",
    "            print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "            msg = (f'NOK - looped at {link} (link idx: {flinks.index(flink)}; '\n",
    "                   f'last idx: {len(flinks)-1})')\n",
    "            break\n",
    "        else:\n",
    "            flinks.append(flink)\n",
    "            print(link)\n",
    "            if link in linkhist:\n",
    "                ldos = linkhist[link]\n",
    "                print(f'Short path found ({link}; DoS = {ldos}).')\n",
    "                dos = -1 if ldos == -1 else len(flinks) - 1 + ldos\n",
    "                if dos == -1:\n",
    "                    print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "                    msg = (f'NOK - short path at {link} '\n",
    "                           f'(last idx: {len(flinks)-1}).')\n",
    "                else:\n",
    "                    print(f'Successfully reached {philo[lang]} '\n",
    "                          f'w/ DoS = {dos}.')\n",
    "                    msg = (f'OK - short path at {link} '\n",
    "                           f'(last idx: {len(flinks)-1}).')\n",
    "                break\n",
    "        time.sleep(1.25)\n",
    "\n",
    "    print(flinks)\n",
    "    if unquote(flinks[-1]) == urljoin(url, target):\n",
    "        dos = len(flinks) - 1\n",
    "    if dos == -1:\n",
    "        dosl = -1*np.ones(len(flinks))\n",
    "    else:\n",
    "        dosl = np.arange(dos, ldos-1, -1)\n",
    "    df = pd.DataFrame({'Links': flinks, 'DoS': dosl})\n",
    "    if not os.path.exists(f'{dir_}'):\n",
    "        os.makedirs(f'{dir_}')\n",
    "    df.to_csv(f'{dir_}links{a:03d}.csv', index=False)\n",
    "    print(f'Crawl result saved to {dir_}links{a:03d}.csv')\n",
    "    return flinks, dosl, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23ad7aa-3575-4b6d-951f-fee000094aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling English Wikipedia...\n",
      "Link:DoS history not loaded - short paths will occur more frequency with more iterations.\n",
      "\n",
      "--- Iter 0 ---\n",
      "/wiki/Struben_Dam_Bird_Sanctuary\n",
      "/wiki/Lynnwood_Glen\n",
      "/wiki/Suburb\n",
      "/wiki/Metropolitan_area\n",
      "/wiki/Urban_area\n",
      "/wiki/Human_settlement\n",
      "/wiki/Geography\n",
      "/wiki/Earth\n",
      "/wiki/Planet\n",
      "/wiki/Hydrostatic_equilibrium\n",
      "/wiki/Fluid_mechanics\n",
      "/wiki/Physics\n",
      "/wiki/Natural_science\n",
      "/wiki/Branches_of_science\n",
      "/wiki/Sciences\n",
      "/wiki/Scientific_method\n",
      "/wiki/Empirical_evidence\n",
      "/wiki/Proposition\n",
      "/wiki/Philosophy_of_language\n",
      "/wiki/Analytic_philosophy\n",
      "/wiki/Contemporary_philosophy\n",
      "/wiki/Western_philosophy\n",
      "/wiki/Philosophy\n",
      "Successfully reached Philosophy w/ DoS = 22.\n",
      "['https://en.wikipedia.org/wiki/Struben_Dam_Bird_Sanctuary', 'https://en.wikipedia.org/wiki/Lynnwood_Glen', 'https://en.wikipedia.org/wiki/Suburb', 'https://en.wikipedia.org/wiki/Metropolitan_area', 'https://en.wikipedia.org/wiki/Urban_area', 'https://en.wikipedia.org/wiki/Human_settlement', 'https://en.wikipedia.org/wiki/Geography', 'https://en.wikipedia.org/wiki/Earth', 'https://en.wikipedia.org/wiki/Planet', 'https://en.wikipedia.org/wiki/Hydrostatic_equilibrium', 'https://en.wikipedia.org/wiki/Fluid_mechanics', 'https://en.wikipedia.org/wiki/Physics', 'https://en.wikipedia.org/wiki/Natural_science', 'https://en.wikipedia.org/wiki/Branches_of_science', 'https://en.wikipedia.org/wiki/Sciences', 'https://en.wikipedia.org/wiki/Scientific_method', 'https://en.wikipedia.org/wiki/Empirical_evidence', 'https://en.wikipedia.org/wiki/Proposition', 'https://en.wikipedia.org/wiki/Philosophy_of_language', 'https://en.wikipedia.org/wiki/Analytic_philosophy', 'https://en.wikipedia.org/wiki/Contemporary_philosophy', 'https://en.wikipedia.org/wiki/Western_philosophy', 'https://en.wikipedia.org/wiki/Philosophy']\n",
      "Crawl result saved to en_res0516b/links000.csv\n",
      "\n",
      "--- Iter 1 ---\n",
      "/wiki/Yi_mein\n",
      "/wiki/Cantonese_cuisine\n",
      "/wiki/Cuisine\n",
      "/wiki/List_of_cooking_techniques\n",
      "/wiki/Cooking\n",
      "/wiki/Science\n",
      "/wiki/Scientific_method\n",
      "Short path found (/wiki/Scientific_method; DoS = 7).\n",
      "Successfully reached Philosophy w/ DoS = 13.\n",
      "['https://en.wikipedia.org/wiki/Yi_mein', 'https://en.wikipedia.org/wiki/Cantonese_cuisine', 'https://en.wikipedia.org/wiki/Cuisine', 'https://en.wikipedia.org/wiki/List_of_cooking_techniques', 'https://en.wikipedia.org/wiki/Cooking', 'https://en.wikipedia.org/wiki/Science', 'https://en.wikipedia.org/wiki/Scientific_method']\n",
      "Crawl result saved to en_res0516b/links001.csv\n",
      "\n",
      "--- Iter 2 ---\n",
      "/wiki/Tony_Harris_(basketball,_born_1967)\n",
      "/wiki/Basketball\n",
      "/wiki/Team_sport\n",
      "/wiki/Sport\n",
      "/wiki/Physical_activity\n",
      "/wiki/Skeletal_muscle\n",
      "/wiki/Organ_(biology)\n",
      "/wiki/Organism\n",
      "/wiki/Life\n",
      "/wiki/Matter\n",
      "/wiki/Classical_physics\n",
      "/wiki/Physics\n",
      "Short path found (/wiki/Physics; DoS = 11).\n",
      "Successfully reached Philosophy w/ DoS = 22.\n",
      "['https://en.wikipedia.org/wiki/Tony_Harris_(basketball,_born_1967)', 'https://en.wikipedia.org/wiki/Basketball', 'https://en.wikipedia.org/wiki/Team_sport', 'https://en.wikipedia.org/wiki/Sport', 'https://en.wikipedia.org/wiki/Physical_activity', 'https://en.wikipedia.org/wiki/Skeletal_muscle', 'https://en.wikipedia.org/wiki/Organ_(biology)', 'https://en.wikipedia.org/wiki/Organism', 'https://en.wikipedia.org/wiki/Life', 'https://en.wikipedia.org/wiki/Matter', 'https://en.wikipedia.org/wiki/Classical_physics', 'https://en.wikipedia.org/wiki/Physics']\n",
      "Crawl result saved to en_res0516b/links002.csv\n",
      "\n",
      "--- Iter 3 ---\n",
      "/wiki/Judson_A._Thigpen\n",
      "/wiki/Mississippi_House_of_Representatives\n",
      "/wiki/Lower_house\n",
      "/wiki/Debate_chamber\n",
      "/wiki/Debate\n",
      "/wiki/Discussion_moderator\n",
      "/wiki/Academic_conference\n",
      "/wiki/Convention_(meeting)\n",
      "/wiki/Industry_(economics)\n",
      "/wiki/Macroeconomics\n",
      "/wiki/Economics\n",
      "/wiki/Social_science\n",
      "/wiki/Branches_of_science\n",
      "Short path found (/wiki/Branches_of_science; DoS = 9).\n",
      "Successfully reached Philosophy w/ DoS = 21.\n",
      "['https://en.wikipedia.org/wiki/Judson_A._Thigpen', 'https://en.wikipedia.org/wiki/Mississippi_House_of_Representatives', 'https://en.wikipedia.org/wiki/Lower_house', 'https://en.wikipedia.org/wiki/Debate_chamber', 'https://en.wikipedia.org/wiki/Debate', 'https://en.wikipedia.org/wiki/Discussion_moderator', 'https://en.wikipedia.org/wiki/Academic_conference', 'https://en.wikipedia.org/wiki/Convention_(meeting)', 'https://en.wikipedia.org/wiki/Industry_(economics)', 'https://en.wikipedia.org/wiki/Macroeconomics', 'https://en.wikipedia.org/wiki/Economics', 'https://en.wikipedia.org/wiki/Social_science', 'https://en.wikipedia.org/wiki/Branches_of_science']\n",
      "Crawl result saved to en_res0516b/links003.csv\n",
      "\n",
      "--- Iter 4 ---\n",
      "/wiki/Fred_Guardineer\n",
      "/wiki/Illustrator\n",
      "/wiki/Artist\n",
      "/wiki/Art\n",
      "/wiki/Human_behavior\n",
      "/wiki/Human\n",
      "/wiki/Species\n",
      "/wiki/Organism\n",
      "Short path found (/wiki/Organism; DoS = 15).\n",
      "Successfully reached Philosophy w/ DoS = 22.\n",
      "['https://en.wikipedia.org/wiki/Fred_Guardineer', 'https://en.wikipedia.org/wiki/Illustrator', 'https://en.wikipedia.org/wiki/Artist', 'https://en.wikipedia.org/wiki/Art', 'https://en.wikipedia.org/wiki/Human_behavior', 'https://en.wikipedia.org/wiki/Human', 'https://en.wikipedia.org/wiki/Species', 'https://en.wikipedia.org/wiki/Organism']\n",
      "Crawl result saved to en_res0516b/links004.csv\n",
      "\n",
      "--- Iter 5 ---\n",
      "/wiki/Four-stroke_power_valve_system\n",
      "/wiki/Four-stroke_engine\n",
      "/wiki/Internal_combustion\n",
      "/wiki/Heat_engine\n",
      "/wiki/Heat\n",
      "/wiki/Thermodynamics\n",
      "/wiki/Physics\n",
      "Short path found (/wiki/Physics; DoS = 11).\n",
      "Successfully reached Philosophy w/ DoS = 17.\n",
      "['https://en.wikipedia.org/wiki/Four-stroke_power_valve_system', 'https://en.wikipedia.org/wiki/Four-stroke_engine', 'https://en.wikipedia.org/wiki/Internal_combustion', 'https://en.wikipedia.org/wiki/Heat_engine', 'https://en.wikipedia.org/wiki/Heat', 'https://en.wikipedia.org/wiki/Thermodynamics', 'https://en.wikipedia.org/wiki/Physics']\n",
      "Crawl result saved to en_res0516b/links005.csv\n",
      "\n",
      "--- Iter 6 ---\n",
      "/wiki/ASC_(musician)\n",
      "/wiki/Del_Mar,_California\n",
      "/wiki/San_Diego_County,_California\n",
      "/wiki/County_(United_States)\n",
      "/wiki/United_States\n",
      "/wiki/Contiguous_United_States\n",
      "/wiki/U.S._state\n",
      "Looped back to /wiki/United_States.\n",
      "Failed to reach Philosophy; DoS = -1.\n",
      "['https://en.wikipedia.org/wiki/ASC_(musician)', 'https://en.wikipedia.org/wiki/Del_Mar,_California', 'https://en.wikipedia.org/wiki/San_Diego_County,_California', 'https://en.wikipedia.org/wiki/County_(United_States)', 'https://en.wikipedia.org/wiki/United_States', 'https://en.wikipedia.org/wiki/Contiguous_United_States', 'https://en.wikipedia.org/wiki/U.S._state']\n",
      "Crawl result saved to en_res0516b/links006.csv\n",
      "\n",
      "--- Iter 7 ---\n",
      "/wiki/Yarmister_barberi\n",
      "/wiki/Clown_beetle\n",
      "/wiki/Family_(biology)\n",
      "/wiki/Taxonomic_rank\n",
      "/wiki/Biology\n",
      "/wiki/Science\n",
      "Short path found (/wiki/Science; DoS = 8).\n",
      "Successfully reached Philosophy w/ DoS = 13.\n",
      "['https://en.wikipedia.org/wiki/Yarmister_barberi', 'https://en.wikipedia.org/wiki/Clown_beetle', 'https://en.wikipedia.org/wiki/Family_(biology)', 'https://en.wikipedia.org/wiki/Taxonomic_rank', 'https://en.wikipedia.org/wiki/Biology', 'https://en.wikipedia.org/wiki/Science']\n",
      "Crawl result saved to en_res0516b/links007.csv\n",
      "\n",
      "--- Iter 8 ---\n",
      "/wiki/Order_of_Research\n",
      "/wiki/Badges_of_honor_in_Iran\n",
      "/wiki/Iran\n",
      "/wiki/West_Asia\n",
      "/wiki/Asia\n",
      "/wiki/Continent\n",
      "/wiki/Geography\n",
      "Short path found (/wiki/Geography; DoS = 16).\n",
      "Successfully reached Philosophy w/ DoS = 22.\n",
      "['https://en.wikipedia.org/wiki/Order_of_Research', 'https://en.wikipedia.org/wiki/Badges_of_honor_in_Iran', 'https://en.wikipedia.org/wiki/Iran', 'https://en.wikipedia.org/wiki/West_Asia', 'https://en.wikipedia.org/wiki/Asia', 'https://en.wikipedia.org/wiki/Continent', 'https://en.wikipedia.org/wiki/Geography']\n",
      "Crawl result saved to en_res0516b/links008.csv\n",
      "\n",
      "--- Iter 9 ---\n",
      "/wiki/Der_Mond\n",
      "/wiki/Opera\n",
      "/wiki/Theatre\n",
      "/wiki/Performing_arts\n",
      "/wiki/The_arts\n",
      "/wiki/Creativity\n",
      "/wiki/Psychology\n",
      "/wiki/Mind\n",
      "/wiki/Thought\n",
      "/wiki/Consciousness\n",
      "/wiki/Awareness\n",
      "/wiki/Philosophy\n",
      "Successfully reached Philosophy w/ DoS = 11.\n",
      "['https://en.wikipedia.org/wiki/Der_Mond', 'https://en.wikipedia.org/wiki/Opera', 'https://en.wikipedia.org/wiki/Theatre', 'https://en.wikipedia.org/wiki/Performing_arts', 'https://en.wikipedia.org/wiki/The_arts', 'https://en.wikipedia.org/wiki/Creativity', 'https://en.wikipedia.org/wiki/Psychology', 'https://en.wikipedia.org/wiki/Mind', 'https://en.wikipedia.org/wiki/Thought', 'https://en.wikipedia.org/wiki/Consciousness', 'https://en.wikipedia.org/wiki/Awareness', 'https://en.wikipedia.org/wiki/Philosophy']\n",
      "Crawl result saved to en_res0516b/links009.csv\n",
      "\n",
      "--- Iter 10 ---\n",
      "/wiki/Manwer_Lal\n",
      "/wiki/National_Assembly_of_Pakistan\n",
      "/wiki/Lower_house\n",
      "Short path found (/wiki/Lower_house; DoS = 19).\n",
      "Successfully reached Philosophy w/ DoS = 21.\n",
      "['https://en.wikipedia.org/wiki/Manwer_Lal', 'https://en.wikipedia.org/wiki/National_Assembly_of_Pakistan', 'https://en.wikipedia.org/wiki/Lower_house']\n",
      "Crawl result saved to en_res0516b/links010.csv\n",
      "\n",
      "--- Iter 11 ---\n",
      "/wiki/Delibal\n",
      "/wiki/%C3%87a%C4%9Fatay_Ulusoy\n",
      "/wiki/Turkish_people\n",
      "/wiki/Turkic_peoples\n",
      "/wiki/Ethnic_group\n",
      "/wiki/People\n",
      "/wiki/Person\n",
      "/wiki/Reason\n",
      "/wiki/Logic\n",
      "/wiki/Logical_reasoning\n",
      "/wiki/Logical_consequence\n",
      "/wiki/Concept\n",
      "/wiki/Abstraction\n",
      "/wiki/Rule_of_inference\n",
      "/wiki/Philosophy_of_logic\n",
      "/wiki/Philosophy\n",
      "Successfully reached Philosophy w/ DoS = 15.\n",
      "['https://en.wikipedia.org/wiki/Delibal', 'https://en.wikipedia.org/wiki/%C3%87a%C4%9Fatay_Ulusoy', 'https://en.wikipedia.org/wiki/Turkish_people', 'https://en.wikipedia.org/wiki/Turkic_peoples', 'https://en.wikipedia.org/wiki/Ethnic_group', 'https://en.wikipedia.org/wiki/People', 'https://en.wikipedia.org/wiki/Person', 'https://en.wikipedia.org/wiki/Reason', 'https://en.wikipedia.org/wiki/Logic', 'https://en.wikipedia.org/wiki/Logical_reasoning', 'https://en.wikipedia.org/wiki/Logical_consequence', 'https://en.wikipedia.org/wiki/Concept', 'https://en.wikipedia.org/wiki/Abstraction', 'https://en.wikipedia.org/wiki/Rule_of_inference', 'https://en.wikipedia.org/wiki/Philosophy_of_logic', 'https://en.wikipedia.org/wiki/Philosophy']\n",
      "Crawl result saved to en_res0516b/links011.csv\n",
      "\n",
      "--- Iter 12 ---\n",
      "/wiki/2007_International_GT_Open\n",
      "/wiki/International_GT_Open\n",
      "/wiki/Grand_tourer\n",
      "/wiki/Car\n",
      "/wiki/Motor_vehicle\n",
      "/wiki/Vehicle\n",
      "/wiki/Machine\n",
      "/wiki/Power_(physics)\n",
      "/wiki/Physics\n",
      "Short path found (/wiki/Physics; DoS = 11).\n",
      "Successfully reached Philosophy w/ DoS = 19.\n",
      "['https://en.wikipedia.org/wiki/2007_International_GT_Open', 'https://en.wikipedia.org/wiki/International_GT_Open', 'https://en.wikipedia.org/wiki/Grand_tourer', 'https://en.wikipedia.org/wiki/Car', 'https://en.wikipedia.org/wiki/Motor_vehicle', 'https://en.wikipedia.org/wiki/Vehicle', 'https://en.wikipedia.org/wiki/Machine', 'https://en.wikipedia.org/wiki/Power_(physics)', 'https://en.wikipedia.org/wiki/Physics']\n",
      "Crawl result saved to en_res0516b/links012.csv\n",
      "\n",
      "--- Iter 13 ---\n",
      "/wiki/United_States_v._Perez\n",
      "/wiki/Supreme_Court_of_the_United_States\n",
      "/wiki/Highest_court\n",
      "/wiki/Jurisdiction\n",
      "/wiki/Right\n",
      "/wiki/Law\n",
      "/wiki/Law_enforcement\n",
      "/wiki/Government\n",
      "/wiki/State_(polity)\n",
      "/wiki/Politics\n",
      "/wiki/Decision-making\n",
      "/wiki/Psychology\n",
      "Short path found (/wiki/Psychology; DoS = 5).\n",
      "Successfully reached Philosophy w/ DoS = 16.\n",
      "['https://en.wikipedia.org/wiki/United_States_v._Perez', 'https://en.wikipedia.org/wiki/Supreme_Court_of_the_United_States', 'https://en.wikipedia.org/wiki/Highest_court', 'https://en.wikipedia.org/wiki/Jurisdiction', 'https://en.wikipedia.org/wiki/Right', 'https://en.wikipedia.org/wiki/Law', 'https://en.wikipedia.org/wiki/Law_enforcement', 'https://en.wikipedia.org/wiki/Government', 'https://en.wikipedia.org/wiki/State_(polity)', 'https://en.wikipedia.org/wiki/Politics', 'https://en.wikipedia.org/wiki/Decision-making', 'https://en.wikipedia.org/wiki/Psychology']\n",
      "Crawl result saved to en_res0516b/links013.csv\n",
      "\n",
      "--- Iter 14 ---\n",
      "/wiki/Leila_Chaher\n",
      "/wiki/Argentine_Chamber_of_Deputies\n",
      "/wiki/Lower_house\n",
      "Short path found (/wiki/Lower_house; DoS = 19).\n",
      "Successfully reached Philosophy w/ DoS = 21.\n",
      "['https://en.wikipedia.org/wiki/Leila_Chaher', 'https://en.wikipedia.org/wiki/Argentine_Chamber_of_Deputies', 'https://en.wikipedia.org/wiki/Lower_house']\n",
      "Crawl result saved to en_res0516b/links014.csv\n",
      "\n",
      "--- Iter 15 ---\n",
      "/wiki/Krystyna_Guzik\n",
      "/wiki/Biathlon\n",
      "/wiki/Winter_sport\n",
      "/wiki/Sport\n",
      "Short path found (/wiki/Sport; DoS = 19).\n",
      "Successfully reached Philosophy w/ DoS = 22.\n",
      "['https://en.wikipedia.org/wiki/Krystyna_Guzik', 'https://en.wikipedia.org/wiki/Biathlon', 'https://en.wikipedia.org/wiki/Winter_sport', 'https://en.wikipedia.org/wiki/Sport']\n",
      "Crawl result saved to en_res0516b/links015.csv\n",
      "\n",
      "--- Iter 16 ---\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Special:Random (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000021AE38E1350>: Failed to establish a new connection: [WinError 10065] A socket operation was attempted to an unreachable host'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 10065] A socket operation was attempted to an unreachable host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000021AE38E1350>: Failed to establish a new connection: [WinError 10065] A socket operation was attempted to an unreachable host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Special:Random (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000021AE38E1350>: Failed to establish a new connection: [WinError 10065] A socket operation was attempted to an unreachable host'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 87\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# urls = ['https://en.wikipedia.org/wiki/Kevin_Bacon',\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#         'https://en.wikipedia.org/wiki/Doom_(2016_video_game)']\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# urls = ['https://en.wikipedia.org/wiki/wiki/Baarbach',\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#         'https://en.wikipedia.org/wiki/wiki/Data']\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# urls = ['https://es.wikipedia.org/wiki/Territorio']\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# urls = ['https://es.wikipedia.org/wiki/Arte']\u001b[39;00m\n\u001b[0;32m     86\u001b[0m urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 87\u001b[0m \u001b[43mcrawl_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasedir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mres0516b/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 66\u001b[0m, in \u001b[0;36mcrawl_iter\u001b[1;34m(basedir, urls, lang, a, b, test)\u001b[0m\n\u001b[0;32m     64\u001b[0m url \u001b[38;5;241m=\u001b[39m urls[i \u001b[38;5;241m-\u001b[39m a] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(urls) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m flinks, dosl, msg \u001b[38;5;241m=\u001b[39m \u001b[43mweb_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinkhist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m data\u001b[38;5;241m.\u001b[39mappend([urlparse(flinks[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mpath, dosl[\u001b[38;5;241m0\u001b[39m], flinks, msg])\n\u001b[0;32m     68\u001b[0m links \u001b[38;5;241m=\u001b[39m [urlparse(flink)\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m flink \u001b[38;5;129;01min\u001b[39;00m flinks]\n",
      "Cell \u001b[1;32mIn[3], line 90\u001b[0m, in \u001b[0;36mweb_crawler\u001b[1;34m(a, dir_, url, lang, linkhist)\u001b[0m\n\u001b[0;32m     59\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWikipedia:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWikipedia talk:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser_talk:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTalk:\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     61\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelp:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelp talk:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProject:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProject talk:\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m }\n\u001b[0;32m     89\u001b[0m s \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n\u001b[1;32m---> 90\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m link \u001b[38;5;241m=\u001b[39m urlparse(response\u001b[38;5;241m.\u001b[39murl)\u001b[38;5;241m.\u001b[39mpath\n\u001b[0;32m     92\u001b[0m flink \u001b[38;5;241m=\u001b[39m urljoin(url, link)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\MLprojects\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Special:Random (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000021AE38E1350>: Failed to establish a new connection: [WinError 10065] A socket operation was attempted to an unreachable host'))"
     ]
    }
   ],
   "source": [
    "def crawl_iter(basedir, urls=list(), lang='en', a=0, b=100, test=False):\n",
    "    \"\"\"\n",
    "    This function iteratively calls the web_crawler function.\n",
    "    It also loads and saves a link:DoS (linkhist) dictionary which shorts\n",
    "    the path to Philosopy in web_crawler, removing redundant crawls from\n",
    "    previous runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    basedir : str\n",
    "        Base directory where output files will be saved.\n",
    "    urls : list, optional\n",
    "        List of specific URLs to crawl. If not provided, random Wikipedia pages \n",
    "        will be crawled.\n",
    "    lang : str, optional\n",
    "        Language code for Wikipedia pages. Defaults to 'en' (English).\n",
    "        Other options: 'es' (Spanish), 'fr' (French), 'de' (German).\n",
    "    a : int, optional\n",
    "        Starting index for iteration. Defaults to 0.\n",
    "    b : int, optional\n",
    "        Ending index for iteration. Defaults to 100.\n",
    "        This index is ignored if 'urls' is passed.\n",
    "    test : bool, optional\n",
    "        If `True`, performs a test run without saving Link:DoS history.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing consolidated results of the crawl.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function saves Link:DoS history in a CSV file for future iterations.\n",
    "    - It consolidates crawl results into a CSV file based on the specified \n",
    "      iteration range.\n",
    "    - Specify `test=True` for a trial run without saving Link:DoS history.\n",
    "    - Any comparison to target and stopwords need to be unquoted.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    df = crawl_iter('output/', lang='en', a=0, b=50)\n",
    "    \"\"\"\n",
    "    langs = {'en': 'English', 'de': 'German', 'es': 'Spanish', 'fr': 'French'}\n",
    "    dir_ = f'{lang}_{basedir}'\n",
    "    print(f'Crawling {langs[lang]} Wikipedia...')\n",
    "    try:\n",
    "        dflhist = pd.read_csv(f'{dir_}linkhist.csv', header=0)\n",
    "        linkhist = dict(zip(dflhist['Link'], dflhist['DoS']))\n",
    "        print(f'Link:DoS history loaded from {dir_}linkhist.csv - short paths '\n",
    "              'from history will be used.')\n",
    "        print(f'History will{\"\" if not test else \" NOT\"} be updated after this '\n",
    "              'run.')\n",
    "        # print(linkhist)\n",
    "    except Exception:\n",
    "        # print(e)\n",
    "        print('Link:DoS history not loaded - short paths will occur more '\n",
    "              'frequency with more iterations.')\n",
    "        linkhist = dict()\n",
    "    if bool(urls):\n",
    "        b = a + len(urls)\n",
    "    data = []\n",
    "    for i in range(a, b):\n",
    "        url = urls[i - a] if bool(urls) else ''\n",
    "        print(f'\\n--- Iter {i} ---')\n",
    "        flinks, dosl, msg = web_crawler(i, dir_, url, lang, linkhist)\n",
    "        data.append([urlparse(flinks[0]).path, dosl[0], flinks, msg])\n",
    "        links = [urlparse(flink).path for flink in flinks]\n",
    "        linkhist.update({link: dos.astype(int) for link, dos in zip(links, dosl)})\n",
    "        if (i-a) % 50 == 49:  # generate save every 50 datapoints\n",
    "            save_conso(data, dir_, a, i)\n",
    "            save_linkhist(linkhist, dir_, a, i)\n",
    "    df = save_conso(data, dir_, a, b-1, fin=True)\n",
    "    # print(linkhist)\n",
    "    if not test:\n",
    "        save_linkhist(linkhist, dir_, a, b-1, fin=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# urls = ['https://en.wikipedia.org/wiki/Kevin_Bacon',\n",
    "#         'https://en.wikipedia.org/wiki/Doom_(2016_video_game)']\n",
    "# urls = ['https://en.wikipedia.org/wiki/wiki/Baarbach',\n",
    "#         'https://en.wikipedia.org/wiki/wiki/Data']\n",
    "# urls = ['https://es.wikipedia.org/wiki/Territorio']\n",
    "# urls = ['https://es.wikipedia.org/wiki/Arte']\n",
    "urls = []\n",
    "crawl_iter(basedir='res0516b/', urls=urls, lang='en', a=0, b=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fea8f-16bb-492f-92df-bf3aa56d41ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e9625-0d0b-48c3-9121-882ae04e2665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d9668f-a0aa-4de9-99c8-c91b9f933e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbe622-be6f-4c39-aaff-7ab23448d5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a15ab-2b0f-403d-8e1f-1ee879ea32fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# diff data\n",
    "\n",
    "basedir = 'res0513_val/'\n",
    "lang = 'en'\n",
    "url = f'https://{lang}.wikipedia.org/'\n",
    "in_path = 'wiki_scraper_results_val.csv'\n",
    "data = []\n",
    "with open(in_path, 'r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "a, b = 11, 100  # start and end index to retrieve\n",
    "df = df.iloc[a: b].reset_index(drop=True)\n",
    "display(df)\n",
    "urls = df['article'].apply(lambda x: urljoin(url, x)).values.tolist()\n",
    "ndf = crawl_iter(basedir=basedir, urls=urls, a=a, b=b)\n",
    "display(ndf)\n",
    "df['nDoS'] = ndf['DoS']\n",
    "df['nPath'] = ndf['Path']\n",
    "df['diffDoS'] = df['nDoS'] - df['DoS']\n",
    "df['diffPath'] = df['nPath'] == df['path']\n",
    "df.to_csv(f'{lang}_{basedir}diff{a:03d}.{b-1:03d}.csv', index=False)\n",
    "print(f'Diff result saved to {lang}_{basedir}diff{a:03d}.{b-1:03d}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8056d0-570d-4fac-b653-0f1368f3884f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data via csv\n",
    "\n",
    "basedir = 'res0515a/'\n",
    "lang = 'es'\n",
    "url = f'https://{lang}.wikipedia.org/wiki/'\n",
    "data = []\n",
    "for i in range(0, 1000):\n",
    "    with open(f'{lang}_{basedir}links{i:03d}.csv') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    start = urlparse(df.iloc[0, 0]).path\n",
    "    dos = df.iloc[0, 1]\n",
    "    if not bool(urlparse(df.iloc[0, 0]).netloc):\n",
    "        path = df.iloc[:, 0].apply(lambda x: urljoin(url, x)).values.tolist()\n",
    "    else:\n",
    "        path = df.iloc[:, 0].values.tolist()\n",
    "    data.append([start, dos, path])\n",
    "n = 1\n",
    "df = pd.DataFrame(data, columns=['Start', 'DoS', 'Path'])\n",
    "df.to_csv(f'{lang}_{basedir}conso{n:02d}.csv', index=False)\n",
    "print(f'Consolidated results saved to {lang}_{basedir}conso{n:02d}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055031dc-4b24-48eb-bf1f-6b03b962d323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
