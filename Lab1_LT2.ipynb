{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510f894c-02f9-4cb8-8b77-0d3dd52936f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd33b392-5d37-45bc-93eb-d701aeb080b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_parent(tag, pfr_list=True):\n",
    "    \"\"\"\n",
    "    Retrieves the parent tag of a given HTML tag.\n",
    "    It immediately returns the tag 'i' if found in the parent-finding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : bs4.element.Tag\n",
    "        The HTML tag whose parent is to be obtained.\n",
    "    pfr_list : bool, optional\n",
    "        If `True`, iterates through the primary tags (p, li, th, td) to find \n",
    "        the parent. If `False`, only returns the immediate parent.\n",
    "        Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parent : bs4.element.Tag or None\n",
    "        The parent tag of the input tag. Returns None if no parent is found.\n",
    "    \"\"\"\n",
    "\n",
    "    parent = tag.parent\n",
    "    if parent and parent.name == 'i':  # italics for exclusion later\n",
    "        return parent\n",
    "    parents = ['p', 'li', 'th', 'td']\n",
    "    if pfr_list:  # if not usual container, just use the first parent\n",
    "        while parent and parent.name not in parents:  # iterate until tag\n",
    "            parent = parent.parent\n",
    "            if parent and parent.name == 'i':  # italics for exclusion later\n",
    "                return parent\n",
    "    return parent\n",
    "\n",
    "\n",
    "def enclosed(tag, pfr_list=True):\n",
    "    \"\"\"\n",
    "    Checks if an HTML tag is enclosed within parentheses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : bs4.element.Tag\n",
    "        The HTML tag to be checked.\n",
    "    pfr_list : bool, optional\n",
    "        If `True`, considers the tag's immediate parent and its ancestors. If \n",
    "        `False`, only considers the immediate parent. Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the tag is enclosed within parentheses, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    parent = get_parent(tag, pfr_list)\n",
    "    if parent is None:\n",
    "        return False  # not enclosed if no parent\n",
    "    if parent.name == 'i':  # enclosed in italic tag\n",
    "        return True\n",
    "    parent_text = str(parent)  # includes the html tags\n",
    "    tag_text = str(tag)  # includes the html tags\n",
    "    idx = parent_text.index(tag_text)\n",
    "    l_cnt = parent_text[:idx].count('(')\n",
    "    r_cnt = parent_text[:idx].count(')')\n",
    "    if l_cnt == 0 or l_cnt - r_cnt != 1:  # left ( not found, or no open left\n",
    "        return False\n",
    "    l_cnt = parent_text[idx+len(tag_text):].count('(')\n",
    "    r_cnt = parent_text[idx+len(tag_text):].count(')')\n",
    "    # print('right >> ' + parent_text[idx+len(tag_text):])\n",
    "    if r_cnt == 0 or r_cnt - l_cnt != 1:  # right ( not found, or no open right\n",
    "        return False\n",
    "    return True  # enclosed\n",
    "\n",
    "\n",
    "def online(alink, url):\n",
    "    \"\"\"\n",
    "    Checks if an anchor link is online and accessible.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alink : bs4.element.Tag\n",
    "        The anchor link tag to be checked.\n",
    "    url : str\n",
    "        The base URL to which the anchor link is appended.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the anchor link is accessible, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(urljoin(url, alink['href']))\n",
    "    except Exception:\n",
    "        return False\n",
    "    if response.status_code == 200:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def wiki(alink, stop_words):\n",
    "    \"\"\"\n",
    "    Checks if an anchor link is a valid Wikipedia link.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alink : bs4.element.Tag\n",
    "        The anchor link tag to be checked.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the anchor link is a valid Wikipedia link, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    link = alink['href']\n",
    "    netloc = urlparse(link).netloc\n",
    "    if netloc == '' or 'wikipedia.org' in netloc:\n",
    "        return not any([word for word in stop_words if word in unquote(link)])\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_link(body, stop_words, url):\n",
    "    \"\"\"\n",
    "    Retrieves the first valid link within the primary tags (p, li, th, td)\n",
    "    of the main body.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body : bs4.element.Tag\n",
    "        The main content of the web page.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "    url : str\n",
    "        The base URL of the web page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link : str\n",
    "        The URL of the first valid link found.\n",
    "    \"\"\"\n",
    "\n",
    "    link = ''\n",
    "    a_str = ('a[href]:not([href^=\"#cite_note\"]):not([href^=\"#\"])'\n",
    "             ':not(.mw-disambig)')\n",
    "    p_str = ['p', 'li', 'th', 'td']\n",
    "    alinks = body.select(', '.join([f'{p} {a_str} ' for p in p_str]))\n",
    "    for alink in alinks:\n",
    "        if (wiki(alink, stop_words) and online(alink, url)\n",
    "                and not enclosed(alink)):\n",
    "            link = alink['href']\n",
    "            break\n",
    "    return link\n",
    "\n",
    "\n",
    "def get_other_link(body, extags, stop_words, url):\n",
    "    \"\"\"\n",
    "    Retrieves a valid link from the main body if no valid links are found\n",
    "    in get_link().\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body : bs4.element.Tag\n",
    "        The main content of the web page.\n",
    "    extags : list\n",
    "        List of extracted tags from the main content to be added back.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "    url : str\n",
    "        The base URL of the web page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link : str\n",
    "        The URL of another valid link found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    IndexError\n",
    "        If no valid links are found.\n",
    "    \"\"\"\n",
    "\n",
    "    link = ''\n",
    "    a_str = ('a[href]:not([href^=\"#cite_note\"]):not([href^=\"#\"]):'\n",
    "             'not(.mw-disambig)')\n",
    "    for extag in extags:  # add back extracted tags for full search\n",
    "        body.append(extag)\n",
    "    alinks = body.select(f'{a_str}')\n",
    "    for alink in alinks:\n",
    "        if (wiki(alink, stop_words) and online(alink, url)\n",
    "                and not enclosed(alink, False)):\n",
    "            link = alink['href']\n",
    "            break\n",
    "    if link == '':  # no valid links in the body\n",
    "        raise IndexError\n",
    "    return link\n",
    "\n",
    "def save_linkhist(linkhist, dir_, a, iter, fin=False):\n",
    "    \"\"\"\n",
    "    Saves the Link:DoS history to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    linkhist : dict\n",
    "        Dictionary containing link:DoS mappings.\n",
    "    dir_ : str\n",
    "        Directory where the CSV file will be saved.\n",
    "    a : int\n",
    "        Starting index used in the filename for identification.\n",
    "    iter : int\n",
    "        Iteration number used in the filename for identification.\n",
    "    fin : bool, optional\n",
    "        If `True`, indicates the final save. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    dflhist = pd.DataFrame(list(linkhist.items()), columns=['Link', 'DoS'])\n",
    "    buname = f'_bu{a:03d}.{iter:03d}' if not fin else ''\n",
    "    dflhist.to_csv(f'{dir_}linkhist{buname}.csv', index=False)\n",
    "    print(f'Link:DoS {\"backup \" if not fin else \"\"}history saved to {dir_}linkhist{buname}.csv')\n",
    "    return None\n",
    "\n",
    "def save_conso(data, dir_, a, iter, fin=False):\n",
    "    \"\"\"\n",
    "    Saves consolidated crawl results to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        List containing crawl data entries.\n",
    "    dir_ : str\n",
    "        Directory where the CSV file will be saved.\n",
    "    a : int\n",
    "        Starting index used in the filename for identification.\n",
    "    iter : int\n",
    "        Iteration number used in the filename for identification.\n",
    "    fin : bool, optional\n",
    "        If `True`, indicates the final save. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or None\n",
    "        Returns a DataFrame if `fin` is True; otherwise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Start', 'DoS', 'Path', 'Msg'])\n",
    "    buname = '_bu' if not fin else ''\n",
    "    df.to_csv(f'{dir_}conso{buname}{a:03d}.{iter:03d}.csv', index=False)\n",
    "    print(f'Consolidated {\"backup \" if not fin else \"\"}results saved to '\n",
    "          f'{dir_}conso{buname}{a:03d}.{iter:03d}.csv')\n",
    "    return df if fin else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afe29b8-3d40-4e8e-95a9-2b7455d29d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def web_crawler(a, dir_, url=None, lang='en', linkhist={}):\n",
    "    \"\"\"\n",
    "    This function starts from a random Wikipedia page and follows the first\n",
    "    page link until it reaches the Philosophy page, a page with no links, or\n",
    "    loops back to a previously visited link.\n",
    "    This function accepts a link:DoS (linkhist) dictionary which shorts the\n",
    "    path to Philosopy, removing redundant crawls from previous runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : int\n",
    "        Index for the web crawl.\n",
    "    dir_ : str\n",
    "        Directory path where the output scrape CSV file will be saved.\n",
    "    url : str, optional\n",
    "        The starting URL for the web crawl. If not provided, a random Wikipedia\n",
    "        page URL will be chosen based on the specified language.\n",
    "    lang : str, optional\n",
    "        The language code for the Wikipedia pages. Defaults to 'en' (English).\n",
    "        Other options: 'es' (Spanish), 'fr' (French), 'de' (German).\n",
    "    linkhist : dict, optional\n",
    "        Dictionary to store link history - used for the short path, reducing\n",
    "        crawl iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    flinks : list\n",
    "        List of visited page URLs during the crawl.\n",
    "    dosl : list\n",
    "        List of degrees of separation (DoS) for each visited page.\n",
    "    msg : str\n",
    "        Message indicating the status of the crawl (e.g., 'OK - normal path' or\n",
    "        'NOK - no links found').\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function excludes certain types of links (e.g., language links,\n",
    "      disambiguation pages, external links) during the crawl.\n",
    "    - It prioritizes links within specific HTML tags and considers the main tex\n",
    "      of the Wikipedia page.\n",
    "    - The DoS represents the number of steps from the starting page to the\n",
    "      Philosophy page.\n",
    "    - It outputs the single scrape result into a CSV file.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    flinks, dosl, msg = web_crawler(1, 'output/', lang='en')\n",
    "    \"\"\"\n",
    "\n",
    "    if not bool(url):\n",
    "        url = f'https://{lang}.wikipedia.org/wiki/Special:Random'\n",
    "    philo = {\n",
    "        'en': 'Philosophy',\n",
    "        'de': 'Philosophie',\n",
    "        'es': 'Psicología',\n",
    "        'fr': 'Philosophie'\n",
    "    }\n",
    "    target = f'https://{lang}.wikipedia.org/wiki/{philo[lang]}'\n",
    "    stop_words = {\n",
    "        'en': ['Wikipedia:', 'Wikipedia talk:', 'User:', 'User_talk:', 'Talk:',\n",
    "               'Help:', 'Help talk:', 'Project:', 'Project talk:',\n",
    "               'Portal:', 'Portal talk:', 'Template:', 'Template talk:',\n",
    "               'File:', 'File talk:', 'Special:', 'index.php',\n",
    "               'Category:', 'Category talk:', 'Template_talk:', 'MOS:', 'Special:'],\n",
    "        'de': ['Wikipedia:', 'Wikipedia Diskussion:', 'Benutzer:', 'Benutzer Diskussion:',\n",
    "               'Diskussion:',\n",
    "               'Hilfe:', 'Hilfe Diskussion:', 'Projekt:',\n",
    "               'Projekt Diskussion:',\n",
    "               'Portal:', 'Portal Diskussion:', 'Vorlage:',\n",
    "               'Vorlage Diskussion:',\n",
    "               'Datei:', 'Datei Diskussion:', 'Spezial:', 'index.php',\n",
    "               'Kategorie:', 'Kategorie Diskussion:', 'Vorlage Diskussion:',\n",
    "               'MOS:', 'Special:'],\n",
    "        'es': ['Wikipedia:', 'Wikipedia discusión:', 'Usuario:', 'Usuario discusión:',\n",
    "               'Discusión:',\n",
    "               'Ayuda:', 'Ayuda discusión:', 'Wikiproyecto:', 'Wikiproyecto discusión:',\n",
    "               'Portal:', 'Portal discusión:', 'Plantilla:', 'Plantilla discusión:',\n",
    "               'Archivo:', 'Archivo discusión:', 'Especial:', 'index.php',\n",
    "               'Categoría:', 'Categoría discusión:', 'Plantilla discusión:', 'WP:',\n",
    "               'Special:'],\n",
    "        'fr': ['Wikipédia:', 'Discussion Wikipédia:', 'Utilisateur:',\n",
    "               'Discussion utilisateur:', 'Discussion:', 'Aide:', 'Discussion aide:',\n",
    "               'Projet:', 'Discussion Projet:', 'Portail:', 'Discussion Portail:',\n",
    "               'Modèle:', 'Discussion modèle:', 'Fichier:', 'Discussion fichier:',\n",
    "               'Spécial:', 'index.php', 'Catégorie:', 'Discussion catégorie:',\n",
    "               'Discussion modèle:', 'MOS:', 'Spécial:']\n",
    "\n",
    "    }\n",
    "    s = requests.Session()\n",
    "    response = s.get(url)\n",
    "    link = urlparse(response.url).path\n",
    "    flink = urljoin(url, link)\n",
    "    flinks = []\n",
    "    flinks.append(flink)\n",
    "    print(link)\n",
    "    ldos = 0\n",
    "    dos = -1\n",
    "    msg = ''\n",
    "    while True:\n",
    "        response = s.get(flink)\n",
    "        soup = bs4.BeautifulSoup(response.text)\n",
    "        body = soup.find('div', id='bodyContent')\n",
    "        for table in body.find_all('table', class_=['infobox', 'sidebar',\n",
    "                                                    'metadata']):\n",
    "            table.extract()\n",
    "        for div in body.find_all('div', class_=['infobox', 'sidebar',\n",
    "                                                'metadata']):\n",
    "            div.extract()\n",
    "        for div in body.find_all('div', role='note'):\n",
    "            div.extract()\n",
    "        extags = []\n",
    "        for extag in body.find_all('div', class_='thumbcaption'):\n",
    "            extags.append(extag.extract())\n",
    "        for extag in body.find_all('figcaption'):\n",
    "            extags.append(extag.extract())\n",
    "        for extag in body.find_all('table', class_=['infobox',\n",
    "                                                    'standings-box']):\n",
    "            extags.append(extag.extract())\n",
    "        try:\n",
    "            link = get_link(body, stop_words[lang], url)\n",
    "            if link == '':  # no valid link within p, li, th, or td tags\n",
    "                link = get_other_link(body, extags, stop_words[lang], url)\n",
    "            flink = urljoin(url, link)\n",
    "        except IndexError:\n",
    "            print('No valid links found in current page.')\n",
    "            print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "            msg = f'NOK - no links at {link} (last idx: {len(flinks)-1})'\n",
    "            break\n",
    "        # link = link.split('wiki/')[-1]\n",
    "        if unquote(flink) == target:\n",
    "            flinks.append(flink)\n",
    "            print(link)\n",
    "            print(f'Successfully reached {philo[lang]} w/ DoS '\n",
    "                  f'= {len(flinks)-1}.')\n",
    "            msg = 'OK - normal path.'\n",
    "            break\n",
    "        elif flink in flinks:\n",
    "            print(f'Looped back to {link}.')\n",
    "            print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "            msg = (f'NOK - looped at {link} (link idx: {flinks.index(flink)}; '\n",
    "                   f'last idx: {len(flinks)-1})')\n",
    "            break\n",
    "        else:\n",
    "            flinks.append(flink)\n",
    "            print(link)\n",
    "            if link in linkhist:\n",
    "                ldos = linkhist[link]\n",
    "                print(f'Short path found ({link}; DoS = {ldos}).')\n",
    "                dos = -1 if ldos == -1 else len(flinks) - 1 + ldos\n",
    "                if dos == -1:\n",
    "                    print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "                    msg = (f'NOK - short path at {link} '\n",
    "                           f'(last idx: {len(flinks)-1}).')\n",
    "                else:\n",
    "                    print(f'Successfully reached {philo[lang]} '\n",
    "                          f'w/ DoS = {dos}.')\n",
    "                    msg = (f'OK - short path at {link} '\n",
    "                           f'(last idx: {len(flinks)-1}).')\n",
    "                break\n",
    "        time.sleep(1.25)\n",
    "\n",
    "    print(flinks)\n",
    "    if unquote(flinks[-1]) == urljoin(url, target):\n",
    "        dos = len(flinks) - 1\n",
    "    if dos == -1:\n",
    "        dosl = -1*np.ones(len(flinks))\n",
    "    else:\n",
    "        dosl = np.arange(dos, ldos-1, -1)\n",
    "    df = pd.DataFrame({'Links': flinks, 'DoS': dosl})\n",
    "    if not os.path.exists(f'{dir_}'):\n",
    "        os.makedirs(f'{dir_}')\n",
    "    df.to_csv(f'{dir_}links{a:03d}.csv', index=False)\n",
    "    print(f'Crawl result saved to {dir_}links{a:03d}.csv')\n",
    "    return flinks, dosl, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad7aa-3575-4b6d-951f-fee000094aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main function to generate data (can be random, or can be from a list of urls)\n",
    "\n",
    "def crawl_iter(basedir, urls=list(), lang='en', a=0, b=100, test=False):\n",
    "    \"\"\"\n",
    "    This function iteratively calls the web_crawler function.\n",
    "    It also loads and saves a link:DoS (linkhist) dictionary which shorts\n",
    "    the path to Philosopy in web_crawler, removing redundant crawls from\n",
    "    previous runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    basedir : str\n",
    "        Base directory where output files will be saved.\n",
    "    urls : list, optional\n",
    "        List of specific URLs to crawl. If not provided, random Wikipedia pages \n",
    "        will be crawled.\n",
    "    lang : str, optional\n",
    "        Language code for Wikipedia pages. Defaults to 'en' (English).\n",
    "        Other options: 'es' (Spanish), 'fr' (French), 'de' (German).\n",
    "    a : int, optional\n",
    "        Starting index for iteration. Defaults to 0.\n",
    "    b : int, optional\n",
    "        Ending index for iteration. Defaults to 100.\n",
    "        This index is ignored if 'urls' is passed.\n",
    "    test : bool, optional\n",
    "        If `True`, performs a test run without saving Link:DoS history.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing consolidated results of the crawl.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function saves Link:DoS history in a CSV file for future iterations.\n",
    "    - It consolidates crawl results into a CSV file based on the specified \n",
    "      iteration range.\n",
    "    - Specify `test=True` for a trial run without saving Link:DoS history.\n",
    "    - Any comparison to target and stopwords need to be unquoted.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    df = crawl_iter('output/', lang='en', a=0, b=50)\n",
    "    \"\"\"\n",
    "    langs = {'en': 'English', 'de': 'German', 'es': 'Spanish', 'fr': 'French'}\n",
    "    dir_ = f'{lang}_{basedir}'\n",
    "    print(f'Crawling {langs[lang]} Wikipedia...')\n",
    "    try:\n",
    "        dflhist = pd.read_csv(f'{dir_}linkhist.csv', header=0)\n",
    "        linkhist = dict(zip(dflhist['Link'], dflhist['DoS']))\n",
    "        print(f'Link:DoS history loaded from {dir_}linkhist.csv - short paths '\n",
    "              'from history will be used.')\n",
    "        print(f'History will{\"\" if not test else \" NOT\"} be updated after this '\n",
    "              'run.')\n",
    "        # print(linkhist)\n",
    "    except Exception:\n",
    "        # print(e)\n",
    "        print('Link:DoS history not loaded - short paths will occur more '\n",
    "              'frequency with more iterations.')\n",
    "        linkhist = dict()\n",
    "    if bool(urls):\n",
    "        b = a + len(urls)\n",
    "    data = []\n",
    "    for i in range(a, b):\n",
    "        url = urls[i - a] if bool(urls) else ''\n",
    "        print(f'\\n--- Iter {i} ---')\n",
    "        flinks, dosl, msg = web_crawler(i, dir_, url, lang, linkhist)\n",
    "        data.append([urlparse(flinks[0]).path, dosl[0], flinks, msg])\n",
    "        links = [urlparse(flink).path for flink in flinks]\n",
    "        linkhist.update({link: dos.astype(int) for link, dos in zip(links, dosl)})\n",
    "        if (i-a) % 50 == 49:  # generate save every 50 datapoints\n",
    "            save_conso(data, dir_, a, i)\n",
    "            save_linkhist(linkhist, dir_, a, i)\n",
    "    df = save_conso(data, dir_, a, b-1, fin=True)\n",
    "    # print(linkhist)\n",
    "    if not test:\n",
    "        save_linkhist(linkhist, dir_, a, b-1, fin=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# urls = ['https://en.wikipedia.org/wiki/Kevin_Bacon',\n",
    "#         'https://en.wikipedia.org/wiki/Doom_(2016_video_game)']\n",
    "# urls = ['https://en.wikipedia.org/wiki/wiki/Baarbach',\n",
    "#         'https://en.wikipedia.org/wiki/wiki/Data']\n",
    "# urls = ['https://es.wikipedia.org/wiki/Territorio']\n",
    "# urls = ['https://es.wikipedia.org/wiki/Arte']\n",
    "urls = []\n",
    "crawl_iter(basedir='res0516a/', urls=urls, lang='es', a=251, b=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a15ab-2b0f-403d-8e1f-1ee879ea32fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# diff data\n",
    "\n",
    "basedir = 'res0513_val/'\n",
    "lang = 'en'\n",
    "url = f'https://{lang}.wikipedia.org/'\n",
    "in_path = 'wiki_scraper_results_val.csv'\n",
    "data = []\n",
    "with open(in_path, 'r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "a, b = 11, 100  # start and end index to retrieve\n",
    "df = df.iloc[a: b].reset_index(drop=True)\n",
    "display(df)\n",
    "urls = df['article'].apply(lambda x: urljoin(url, x)).values.tolist()\n",
    "ndf = crawl_iter(basedir=basedir, urls=urls, a=a, b=b)\n",
    "display(ndf)\n",
    "df['nDoS'] = ndf['DoS']\n",
    "df['nPath'] = ndf['Path']\n",
    "df['diffDoS'] = df['nDoS'] - df['DoS']\n",
    "df['diffPath'] = df['nPath'] == df['path']\n",
    "df.to_csv(f'{lang}_{basedir}diff{a:03d}.{b-1:03d}.csv', index=False)\n",
    "print(f'Diff result saved to {lang}_{basedir}diff{a:03d}.{b-1:03d}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a8056d0-570d-4fac-b653-0f1368f3884f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T11:56:15.265755Z",
     "iopub.status.busy": "2024-05-07T11:56:15.265026Z",
     "iopub.status.idle": "2024-05-07T11:56:15.307131Z",
     "shell.execute_reply": "2024-05-07T11:56:15.305370Z",
     "shell.execute_reply.started": "2024-05-07T11:56:15.265691Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated results saved to es_res0515a/conso01.csv\n"
     ]
    }
   ],
   "source": [
    "# data via csv\n",
    "\n",
    "basedir = 'res0515a/'\n",
    "lang = 'es'\n",
    "url = f'https://{lang}.wikipedia.org/wiki/'\n",
    "data = []\n",
    "for i in range(0, 1000):\n",
    "    with open(f'{lang}_{basedir}links{i:03d}.csv') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    start = urlparse(df.iloc[0, 0]).path\n",
    "    dos = df.iloc[0, 1]\n",
    "    if not bool(urlparse(df.iloc[0, 0]).netloc):\n",
    "        path = df.iloc[:, 0].apply(lambda x: urljoin(url, x)).values.tolist()\n",
    "    else:\n",
    "        path = df.iloc[:, 0].values.tolist()\n",
    "    data.append([start, dos, path])\n",
    "n = 1\n",
    "df = pd.DataFrame(data, columns=['Start', 'DoS', 'Path'])\n",
    "df.to_csv(f'{lang}_{basedir}conso{n:02d}.csv', index=False)\n",
    "print(f'Consolidated results saved to {lang}_{basedir}conso{n:02d}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055031dc-4b24-48eb-bf1f-6b03b962d323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
