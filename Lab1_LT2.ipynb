{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "510f894c-02f9-4cb8-8b77-0d3dd52936f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd33b392-5d37-45bc-93eb-d701aeb080b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_parent(tag, pfr_list=True):\n",
    "    \"\"\"\n",
    "    Retrieves the parent tag of a given HTML tag.\n",
    "    It immediately returns the tag 'i' if found in the parent-finding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : bs4.element.Tag\n",
    "        The HTML tag whose parent is to be obtained.\n",
    "    pfr_list : bool, optional\n",
    "        If `True`, iterates through the primary tags (p, li, th, td) to find \n",
    "        the parent. If `False`, only returns the immediate parent.\n",
    "        Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parent : bs4.element.Tag or None\n",
    "        The parent tag of the input tag. Returns None if no parent is found.\n",
    "    \"\"\"\n",
    "\n",
    "    parent = tag.parent\n",
    "    if parent and parent.name == 'i':  # italics for exclusion later\n",
    "        return parent\n",
    "    parents = ['p', 'li', 'th', 'td']\n",
    "    if pfr_list:  # if not usual container, just use the first parent\n",
    "        while parent and parent.name not in parents:  # iterate until tag\n",
    "            parent = parent.parent\n",
    "            if parent and parent.name == 'i':  # italics for exclusion later\n",
    "                return parent\n",
    "    return parent\n",
    "\n",
    "\n",
    "def enclosed(tag, pfr_list=True):\n",
    "    \"\"\"\n",
    "    Checks if an HTML tag is enclosed within parentheses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : bs4.element.Tag\n",
    "        The HTML tag to be checked.\n",
    "    pfr_list : bool, optional\n",
    "        If `True`, considers the tag's immediate parent and its ancestors. If \n",
    "        `False`, only considers the immediate parent. Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the tag is enclosed within parentheses, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    parent = get_parent(tag, pfr_list)\n",
    "    if parent is None:\n",
    "        return False  # not enclosed if no parent\n",
    "    if parent.name == 'i':  # enclosed in italic tag\n",
    "        return True\n",
    "    parent_text = str(parent)  # includes the html tags\n",
    "    tag_text = str(tag)  # includes the html tags\n",
    "    idx = parent_text.index(tag_text)\n",
    "    l_cnt = parent_text[:idx].count('(')\n",
    "    r_cnt = parent_text[:idx].count(')')\n",
    "    if l_cnt == 0 or l_cnt - r_cnt != 1:  # left ( not found, or no open left\n",
    "        return False\n",
    "    l_cnt = parent_text[idx+len(tag_text):].count('(')\n",
    "    r_cnt = parent_text[idx+len(tag_text):].count(')')\n",
    "    # print('right >> ' + parent_text[idx+len(tag_text):])\n",
    "    if r_cnt == 0 or r_cnt - l_cnt != 1:  # right ( not found, or no open right\n",
    "        return False\n",
    "    return True  # enclosed\n",
    "\n",
    "\n",
    "def online(alink, url):\n",
    "    \"\"\"\n",
    "    Checks if an anchor link is online and accessible.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alink : bs4.element.Tag\n",
    "        The anchor link tag to be checked.\n",
    "    url : str\n",
    "        The base URL to which the anchor link is appended.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the anchor link is accessible, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(urljoin(url, alink['href']))\n",
    "    except Exception:\n",
    "        return False\n",
    "    if response.status_code == 200:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def wiki(alink, stop_words):\n",
    "    \"\"\"\n",
    "    Checks if an anchor link is a valid Wikipedia link.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alink : bs4.element.Tag\n",
    "        The anchor link tag to be checked.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the anchor link is a valid Wikipedia link, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    link = alink['href']\n",
    "    netloc = urlparse(link).netloc\n",
    "    if netloc == '' or 'wikipedia.org' in netloc:\n",
    "        return not any([word for word in stop_words if word in unquote(link)])\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_link(body, stop_words, url):\n",
    "    \"\"\"\n",
    "    Retrieves the first valid link within the primary tags (p, li, th, td)\n",
    "    of the main body.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body : bs4.element.Tag\n",
    "        The main content of the web page.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "    url : str\n",
    "        The base URL of the web page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link : str\n",
    "        The URL of the first valid link found.\n",
    "    \"\"\"\n",
    "\n",
    "    link = ''\n",
    "    a_str = ('a[href]:not([href^=\"#cite_note\"]):not([href^=\"#\"])'\n",
    "             ':not(.mw-disambig)')\n",
    "    p_str = ['p', 'li', 'th', 'td']\n",
    "    alinks = body.select(', '.join([f'{p} {a_str} ' for p in p_str]))\n",
    "    for alink in alinks:\n",
    "        if (wiki(alink, stop_words) and online(alink, url)\n",
    "                and not enclosed(alink)):\n",
    "            link = alink['href']\n",
    "            break\n",
    "    return link\n",
    "\n",
    "\n",
    "def get_other_link(body, extags, stop_words, url):\n",
    "    \"\"\"\n",
    "    Retrieves a valid link from the main body if no valid links are found\n",
    "    in get_link().\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body : bs4.element.Tag\n",
    "        The main content of the web page.\n",
    "    extags : list\n",
    "        List of extracted tags from the main content to be added back.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "    url : str\n",
    "        The base URL of the web page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link : str\n",
    "        The URL of another valid link found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    IndexError\n",
    "        If no valid links are found.\n",
    "    \"\"\"\n",
    "\n",
    "    link = ''\n",
    "    a_str = ('a[href]:not([href^=\"#cite_note\"]):not([href^=\"#\"]):'\n",
    "             'not(.mw-disambig)')\n",
    "    for extag in extags:  # add back extracted tags for full search\n",
    "        body.append(extag)\n",
    "    alinks = body.select(f'{a_str}')\n",
    "    for alink in alinks:\n",
    "        if (wiki(alink, stop_words) and online(alink, url)\n",
    "                and not enclosed(alink, False)):\n",
    "            link = alink['href']\n",
    "            break\n",
    "    if link == '':  # no valid links in the body\n",
    "        raise IndexError\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2afe29b8-3d40-4e8e-95a9-2b7455d29d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def web_crawler(a, dir_, url=None, lang='en', linkhist={}):\n",
    "    \"\"\"\n",
    "    This function starts from a random Wikipedia page and follows the first\n",
    "    page link until it reaches the Philosophy page, a page with no links, or\n",
    "    loops back to a previously visited link.\n",
    "    This function accepts a link:DoS (linkhist) dictionary which shorts the\n",
    "    path to Philosopy, removing redundant crawls from previous runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : int\n",
    "        Index for the web crawl.\n",
    "    dir_ : str\n",
    "        Directory path where the output scrape CSV file will be saved.\n",
    "    url : str, optional\n",
    "        The starting URL for the web crawl. If not provided, a random Wikipedia\n",
    "        page URL will be chosen based on the specified language.\n",
    "    lang : str, optional\n",
    "        The language code for the Wikipedia pages. Defaults to 'en' (English).\n",
    "        Another option is 'de' (German).\n",
    "    linkhist : dict, optional\n",
    "        Dictionary to store link history - used for the short path, reducing\n",
    "        crawl iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    flinks : list\n",
    "        List of visited page URLs during the crawl.\n",
    "    dosl : list\n",
    "        List of degrees of separation (DoS) for each visited page.\n",
    "    msg : str\n",
    "        Message indicating the status of the crawl (e.g., 'OK - normal path' or\n",
    "        'NOK - no links found').\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function excludes certain types of links (e.g., language links,\n",
    "      disambiguation pages, external links) during the crawl.\n",
    "    - It prioritizes links within specific HTML tags and considers the main tex\n",
    "      of the Wikipedia page.\n",
    "    - The DoS represents the number of steps from the starting page to the\n",
    "      Philosophy page.\n",
    "    - It outputs the single scrape result into a CSV file.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    flinks, dosl, msg = web_crawler(1, 'output/', lang='en')\n",
    "    \"\"\"\n",
    "\n",
    "    if not bool(url):\n",
    "        url = f'https://{lang}.wikipedia.org/wiki/Special:Random'\n",
    "    philo = {\n",
    "        'en': 'Philosophy',\n",
    "        'de': 'Philosophie',\n",
    "        'es': 'Filosofía'\n",
    "    }\n",
    "    target = f'https://{lang}.wikipedia.org/wiki/{philo[lang]}'\n",
    "    stop_words = {\n",
    "        'en': ['Wikipedia:', 'Wikipedia talk:', 'User_talk:', 'Talk:',\n",
    "               'Help:', 'Help talk:', 'Project:', 'Project talk:',\n",
    "               'Portal:', 'Portal talk:', 'Template:', 'Template talk:',\n",
    "               'File:', 'File talk:', 'Special:', 'index.php',\n",
    "               'Category:', 'Category talk:', 'Template_talk:', 'MOS:'],\n",
    "        'de': ['Wikipedia:', 'Wikipedia Diskussion:', 'Benutzer Diskussion:',\n",
    "               'Diskussion:',\n",
    "               'Hilfe:', 'Hilfe Diskussion:', 'Projekt:',\n",
    "               'Projekt Diskussion:',\n",
    "               'Portal:', 'Portal Diskussion:', 'Vorlage:',\n",
    "               'Vorlage Diskussion:',\n",
    "               'Datei:', 'Datei Diskussion:', 'Spezial:', 'index.php',\n",
    "               'Kategorie:', 'Kategorie Diskussion:', 'Vorlage Diskussion:',\n",
    "               'MOS:'],\n",
    "        'es': ['Wikipedia:', 'Wikipedia discusión:', 'Usuario discusión:', 'Discusión:',\n",
    "                'Ayuda:', 'Ayuda discusión:', 'Wikiproyecto:', 'Wikiproyecto discusión:',\n",
    "                'Portal:', 'Portal discusión:', 'Plantilla:', 'Plantilla discusión:',\n",
    "                'Archivo:', 'Archivo discusión:', 'Especial:', 'index.php',\n",
    "                'Categoría:', 'Categoría discusión:', 'Plantilla discusión:', 'WP:',\n",
    "                'Special:']\n",
    "    }\n",
    "    s = requests.Session()\n",
    "    response = s.get(url)\n",
    "    link = urlparse(response.url).path\n",
    "    flink = urljoin(url, link)\n",
    "    flinks = []\n",
    "    flinks.append(flink)\n",
    "    print(link)\n",
    "    ldos = 0\n",
    "    dos = -1\n",
    "    msg = ''\n",
    "    while True:\n",
    "        response = s.get(flink)\n",
    "        soup = bs4.BeautifulSoup(response.text)\n",
    "        body = soup.find('div', id='bodyContent')\n",
    "        for table in body.find_all('table', class_=['infobox', 'sidebar',\n",
    "                                                    'metadata']):\n",
    "            table.extract()\n",
    "        for div in body.find_all('div', role='note'):\n",
    "            div.extract()\n",
    "        extags = []\n",
    "        for extag in body.find_all('div', class_='thumbcaption'):\n",
    "            extags.append(extag.extract())\n",
    "        for extag in body.find_all('figcaption'):\n",
    "            extags.append(extag.extract())\n",
    "        for extag in body.find_all('table', class_=['infobox',\n",
    "                                                    'standings-box']):\n",
    "            extags.append(extag.extract())\n",
    "        try:\n",
    "            link = get_link(body, stop_words[lang], url)\n",
    "            if link == '':  # no valid link within p, li, th, or td tags\n",
    "                link = get_other_link(body, extags, stop_words[lang], url)\n",
    "            flink = urljoin(url, link)\n",
    "        except IndexError:\n",
    "            print('No valid links found in current page.')\n",
    "            print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "            msg = f'NOK - no links at {link} (last idx: {len(flinks)-1})'\n",
    "            break\n",
    "        # link = link.split('wiki/')[-1]\n",
    "        if unquote(flink) == target:\n",
    "            flinks.append(flink)\n",
    "            print(link)\n",
    "            print(f'Successfully reached {philo[lang]} w/ DoS '\n",
    "                  f'= {len(flinks)-1}.')\n",
    "            msg = 'OK - normal path.'\n",
    "            break\n",
    "        elif flink in flinks:\n",
    "            print(f'Looped back to {link}.')\n",
    "            print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "            msg = (f'NOK - looped at {link} (link idx: {flinks.index(flink)}; '\n",
    "                   f'last idx: {len(flinks)-1})')\n",
    "            break\n",
    "        else:\n",
    "            flinks.append(flink)\n",
    "            print(link)\n",
    "            if link in linkhist:\n",
    "                ldos = linkhist[link]\n",
    "                print(f'Short path found ({link}; DoS = {ldos}).')\n",
    "                dos = -1 if ldos == -1 else len(flinks) - 1 + ldos\n",
    "                if dos == -1:\n",
    "                    print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "                    msg = (f'NOK - short path at {link} '\n",
    "                           f'(last idx: {len(flinks)-1}).')\n",
    "                else:\n",
    "                    print(f'Successfully reached {philo[lang]} '\n",
    "                          f'w/ DoS = {dos}.')\n",
    "                    msg = (f'OK - short path at {link} '\n",
    "                           f'(last idx: {len(flinks)-1}).')\n",
    "                break\n",
    "        time.sleep(1.25)\n",
    "\n",
    "    print(flinks)\n",
    "    if unquote(flinks[-1]) == urljoin(url, target):\n",
    "        dos = len(flinks) - 1\n",
    "    if dos == -1:\n",
    "        dosl = -1*np.ones(len(flinks))\n",
    "    else:\n",
    "        dosl = np.arange(dos, ldos-1, -1)\n",
    "    df = pd.DataFrame({'Links': flinks, 'DoS': dosl})\n",
    "    if not os.path.exists(f'{dir_}'):\n",
    "        os.makedirs(f'{dir_}')\n",
    "    df.to_csv(f'{dir_}links{a:03d}.csv', index=False)\n",
    "    print(f'Crawl result saved to {dir_}links{a:03d}.csv')\n",
    "    return flinks, dosl, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f23ad7aa-3575-4b6d-951f-fee000094aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling Spanish Wikipedia...\n",
      "Link:DoS history loaded from es_res0514a/linkhist.csv - short paths from history will be used.\n",
      "History will be updated after this run.\n",
      "\n",
      "--- Iter 0 ---\n",
      "/wiki/Robertus_Cornelis_Hilarius_Maria_Oudejans\n",
      "/wiki/Bot%C3%A1nico\n",
      "/wiki/Biolog%C3%ADa\n",
      "/wiki/Ciencia_natural\n",
      "/wiki/Ciencia\n",
      "/wiki/Sistema\n",
      "/wiki/Concepto\n",
      "/wiki/Entendimiento\n",
      "/wiki/Aptitud\n",
      "/wiki/Psicolog%C3%ADa\n",
      "Looped back to /wiki/Ciencia.\n",
      "Failed to reach Filosofía; DoS = -1.\n",
      "['https://es.wikipedia.org/wiki/Robertus_Cornelis_Hilarius_Maria_Oudejans', 'https://es.wikipedia.org/wiki/Bot%C3%A1nico', 'https://es.wikipedia.org/wiki/Biolog%C3%ADa', 'https://es.wikipedia.org/wiki/Ciencia_natural', 'https://es.wikipedia.org/wiki/Ciencia', 'https://es.wikipedia.org/wiki/Sistema', 'https://es.wikipedia.org/wiki/Concepto', 'https://es.wikipedia.org/wiki/Entendimiento', 'https://es.wikipedia.org/wiki/Aptitud', 'https://es.wikipedia.org/wiki/Psicolog%C3%ADa']\n",
      "Crawl result saved to es_res0514a/links000.csv\n",
      "\n",
      "--- Iter 1 ---\n",
      "/wiki/Campeonato_de_Wimbledon_1904\n",
      "/wiki/Campeonato_de_Wimbledon\n",
      "/wiki/All_England_Lawn_Tennis_and_Croquet_Club\n",
      "/wiki/Club\n",
      "/wiki/Club_de_f%C3%BAtbol\n",
      "/wiki/F%C3%BAtbol\n",
      "/wiki/Deporte_de_equipo\n",
      "/wiki/F%C3%BAtbol_en_los_Juegos_Ol%C3%ADmpicos\n",
      "Looped back to /wiki/F%C3%BAtbol.\n",
      "Failed to reach Filosofía; DoS = -1.\n",
      "['https://es.wikipedia.org/wiki/Campeonato_de_Wimbledon_1904', 'https://es.wikipedia.org/wiki/Campeonato_de_Wimbledon', 'https://es.wikipedia.org/wiki/All_England_Lawn_Tennis_and_Croquet_Club', 'https://es.wikipedia.org/wiki/Club', 'https://es.wikipedia.org/wiki/Club_de_f%C3%BAtbol', 'https://es.wikipedia.org/wiki/F%C3%BAtbol', 'https://es.wikipedia.org/wiki/Deporte_de_equipo', 'https://es.wikipedia.org/wiki/F%C3%BAtbol_en_los_Juegos_Ol%C3%ADmpicos']\n",
      "Crawl result saved to es_res0514a/links001.csv\n",
      "\n",
      "--- Iter 2 ---\n",
      "/wiki/Viva_el_verano_(programa_de_televisi%C3%B3n)\n",
      "/wiki/Programa_(difusi%C3%B3n)\n",
      "/wiki/Radiodifusi%C3%B3n\n",
      "/wiki/Radio_(medio_de_comunicaci%C3%B3n)\n",
      "/wiki/Medio_de_comunicaci%C3%B3n\n",
      "/wiki/Medio_de_comunicaci%C3%B3n_de_masas\n",
      "Looped back to /wiki/Medio_de_comunicaci%C3%B3n.\n",
      "Failed to reach Filosofía; DoS = -1.\n",
      "['https://es.wikipedia.org/wiki/Viva_el_verano_(programa_de_televisi%C3%B3n)', 'https://es.wikipedia.org/wiki/Programa_(difusi%C3%B3n)', 'https://es.wikipedia.org/wiki/Radiodifusi%C3%B3n', 'https://es.wikipedia.org/wiki/Radio_(medio_de_comunicaci%C3%B3n)', 'https://es.wikipedia.org/wiki/Medio_de_comunicaci%C3%B3n', 'https://es.wikipedia.org/wiki/Medio_de_comunicaci%C3%B3n_de_masas']\n",
      "Crawl result saved to es_res0514a/links002.csv\n",
      "\n",
      "--- Iter 3 ---\n",
      "/wiki/Baliosus_terminatus\n",
      "/wiki/Coleoptera\n",
      "/wiki/Orden_(biolog%C3%ADa)\n",
      "/wiki/Biolog%C3%ADa\n",
      "Short path found (/wiki/Biolog%C3%ADa; DoS = -1).\n",
      "Failed to reach Filosofía; DoS = -1.\n",
      "['https://es.wikipedia.org/wiki/Baliosus_terminatus', 'https://es.wikipedia.org/wiki/Coleoptera', 'https://es.wikipedia.org/wiki/Orden_(biolog%C3%ADa)', 'https://es.wikipedia.org/wiki/Biolog%C3%ADa']\n",
      "Crawl result saved to es_res0514a/links003.csv\n",
      "\n",
      "--- Iter 4 ---\n",
      "/wiki/Condado_de_Otoe\n",
      "/wiki/Otoe\n",
      "/wiki/Nativos_americanos\n",
      "/wiki/Am%C3%A9rica\n",
      "/wiki/Continente\n",
      "/wiki/Territorio\n",
      "/wiki/%C3%81rea\n",
      "/wiki/Espacio_m%C3%A9trico\n",
      "/wiki/Matem%C3%A1tica\n",
      "/wiki/Ciencias_formales\n",
      "/wiki/Ciencia\n",
      "Short path found (/wiki/Ciencia; DoS = -1).\n",
      "Failed to reach Filosofía; DoS = -1.\n",
      "['https://es.wikipedia.org/wiki/Condado_de_Otoe', 'https://es.wikipedia.org/wiki/Otoe', 'https://es.wikipedia.org/wiki/Nativos_americanos', 'https://es.wikipedia.org/wiki/Am%C3%A9rica', 'https://es.wikipedia.org/wiki/Continente', 'https://es.wikipedia.org/wiki/Territorio', 'https://es.wikipedia.org/wiki/%C3%81rea', 'https://es.wikipedia.org/wiki/Espacio_m%C3%A9trico', 'https://es.wikipedia.org/wiki/Matem%C3%A1tica', 'https://es.wikipedia.org/wiki/Ciencias_formales', 'https://es.wikipedia.org/wiki/Ciencia']\n",
      "Crawl result saved to es_res0514a/links004.csv\n",
      "\n",
      "Consolidated results saved to es_res0514a/conso000.004.csv\n",
      "Link:DoS history saved to es_res0514a/linkhist.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>DoS</th>\n",
       "      <th>Path</th>\n",
       "      <th>Msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/wiki/Robertus_Cornelis_Hilarius_Maria_Oudejans</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Robertus_Cornel...</td>\n",
       "      <td>NOK - looped at /wiki/Ciencia (link idx: 4; la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/wiki/Campeonato_de_Wimbledon_1904</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Campeonato_de_W...</td>\n",
       "      <td>NOK - looped at /wiki/F%C3%BAtbol (link idx: 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/wiki/Viva_el_verano_(programa_de_televisi%C3%...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Viva_el_verano_...</td>\n",
       "      <td>NOK - looped at /wiki/Medio_de_comunicaci%C3%B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/wiki/Baliosus_terminatus</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Baliosus_termin...</td>\n",
       "      <td>NOK - short path at /wiki/Biolog%C3%ADa (last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/wiki/Condado_de_Otoe</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Condado_de_Otoe...</td>\n",
       "      <td>NOK - short path at /wiki/Ciencia (last idx: 10).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Start  DoS  \\\n",
       "0    /wiki/Robertus_Cornelis_Hilarius_Maria_Oudejans -1.0   \n",
       "1                 /wiki/Campeonato_de_Wimbledon_1904 -1.0   \n",
       "2  /wiki/Viva_el_verano_(programa_de_televisi%C3%... -1.0   \n",
       "3                          /wiki/Baliosus_terminatus -1.0   \n",
       "4                              /wiki/Condado_de_Otoe -1.0   \n",
       "\n",
       "                                                Path  \\\n",
       "0  [https://es.wikipedia.org/wiki/Robertus_Cornel...   \n",
       "1  [https://es.wikipedia.org/wiki/Campeonato_de_W...   \n",
       "2  [https://es.wikipedia.org/wiki/Viva_el_verano_...   \n",
       "3  [https://es.wikipedia.org/wiki/Baliosus_termin...   \n",
       "4  [https://es.wikipedia.org/wiki/Condado_de_Otoe...   \n",
       "\n",
       "                                                 Msg  \n",
       "0  NOK - looped at /wiki/Ciencia (link idx: 4; la...  \n",
       "1  NOK - looped at /wiki/F%C3%BAtbol (link idx: 5...  \n",
       "2  NOK - looped at /wiki/Medio_de_comunicaci%C3%B...  \n",
       "3  NOK - short path at /wiki/Biolog%C3%ADa (last ...  \n",
       "4  NOK - short path at /wiki/Ciencia (last idx: 10).  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main function to generate data (can be random, or can be from a list of urls)\n",
    "\n",
    "def crawl_iter(basedir, urls=list(), lang='en', a=0, b=100, test=False):\n",
    "    \"\"\"\n",
    "    This function iteratively calls the web_crawler function.\n",
    "    It also loads and saves a link:DoS (linkhist) dictionary which shorts\n",
    "    the path to Philosopy in web_crawler, removing redundant crawls from\n",
    "    previous runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    basedir : str\n",
    "        Base directory where output files will be saved.\n",
    "    urls : list, optional\n",
    "        List of specific URLs to crawl. If not provided, random Wikipedia pages \n",
    "        will be crawled.\n",
    "    lang : str, optional\n",
    "        Language code for Wikipedia pages. Defaults to 'en' (English).\n",
    "        Another option is 'de' (German).\n",
    "    a : int, optional\n",
    "        Starting index for iteration. Defaults to 0.\n",
    "    b : int, optional\n",
    "        Ending index for iteration. Defaults to 100.\n",
    "        This index is ignored if 'urls' is passed.\n",
    "    test : bool, optional\n",
    "        If `True`, performs a test run without saving Link:DoS history.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing consolidated results of the crawl.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function saves Link:DoS history in a CSV file for future iterations.\n",
    "    - It consolidates crawl results into a CSV file based on the specified \n",
    "      iteration range.\n",
    "    - Specify `test=True` for a trial run without saving Link:DoS history.\n",
    "    - Any comparison to target and stopwords need to be unquoted.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    df = crawl_iter('output/', lang='en', a=0, b=50)\n",
    "    \"\"\"\n",
    "    langs = {'en': 'English', 'de': 'German', 'es': 'Spanish'}\n",
    "    dir_ = f'{lang}_{basedir}'\n",
    "    print(f'Crawling {langs[lang]} Wikipedia...')\n",
    "    try:\n",
    "        dflhist = pd.read_csv(f'{dir_}linkhist.csv', header=0)\n",
    "        linkhist = dict(zip(dflhist['Link'], dflhist['DoS']))\n",
    "        print(f'Link:DoS history loaded from {dir_}linkhist.csv - short paths '\n",
    "              'from history will be used.')\n",
    "        print(f'History will{\"\" if not test else \" NOT\"} be updated after this '\n",
    "              'run.')\n",
    "        # print(linkhist)\n",
    "    except Exception:\n",
    "        # print(e)\n",
    "        print('Link:DoS history not loaded - short paths will occur more '\n",
    "              'frequency with more iterations.')\n",
    "        linkhist = dict()\n",
    "    if bool(urls):\n",
    "        b = a + len(urls)\n",
    "    data = []\n",
    "    for i in range(a, b):\n",
    "        url = urls[i - a] if bool(urls) else ''\n",
    "        print(f'\\n--- Iter {i} ---')\n",
    "        flinks, dosl, msg = web_crawler(i, dir_, url, lang, linkhist)\n",
    "        data.append([urlparse(flinks[0]).path, dosl[0], flinks, msg])\n",
    "        links = [urlparse(flink).path for flink in flinks]\n",
    "        linkhist.update({link: dos.astype(int) for link, dos in zip(links, dosl)})\n",
    "    df = pd.DataFrame(data, columns=['Start', 'DoS', 'Path', 'Msg'])\n",
    "    df.to_csv(f'{dir_}conso{a:03d}.{b-1:03d}.csv', index=False)\n",
    "    print(f'\\nConsolidated results saved to {dir_}conso{a:03d}.{b-1:03d}.csv')\n",
    "    # print(linkhist)\n",
    "    if not test:\n",
    "        dflhist = pd.DataFrame(list(linkhist.items()), columns=['Link', 'DoS'])\n",
    "        dflhist.to_csv(f'{dir_}linkhist.csv', index=False)\n",
    "        print(f'Link:DoS history saved to {dir_}linkhist.csv')\n",
    "    return df\n",
    "\n",
    "\n",
    "# urls = ['https://en.wikipedia.org/wiki/Kevin_Bacon',\n",
    "#         'https://en.wikipedia.org/wiki/Doom_(2016_video_game)']\n",
    "# urls = ['https://en.wikipedia.org/wiki/wiki/Baarbach',\n",
    "#         'https://en.wikipedia.org/wiki/wiki/Data']\n",
    "# urls = ['https://es.wikipedia.org/wiki/Territorio']\n",
    "# urls = ['https://es.wikipedia.org/wiki/Arte']\n",
    "urls = []\n",
    "crawl_iter(basedir='res0514a/', urls=urls, lang='es', a=0, b=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a15ab-2b0f-403d-8e1f-1ee879ea32fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# diff data\n",
    "\n",
    "url = 'https://en.wikipedia.org/'\n",
    "basedir = 'res0513_val/'\n",
    "in_path = 'wiki_scraper_results_val.csv'\n",
    "data = []\n",
    "with open(in_path, 'r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "a, b = 11, 100  # start and end index to retrieve\n",
    "df = df.iloc[a: b].reset_index(drop=True)\n",
    "display(df)\n",
    "urls = df['article'].apply(lambda x: urljoin(url, x)).values.tolist()\n",
    "ndf = crawl_iter(basedir=basedir, urls=urls, a=a, b=b)\n",
    "display(ndf)\n",
    "df['nDoS'] = ndf['DoS']\n",
    "df['nPath'] = ndf['Path']\n",
    "df['diffDoS'] = df['nDoS'] - df['DoS']\n",
    "df['diffPath'] = df['nPath'] == df['path']\n",
    "df.to_csv(f'en_{basedir}diff{a:03d}.{b-1:03d}.csv', index=False)\n",
    "print(f'Diff result saved to en_{basedir}diff{a:03d}.{b-1:03d}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a8056d0-570d-4fac-b653-0f1368f3884f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T11:56:15.265755Z",
     "iopub.status.busy": "2024-05-07T11:56:15.265026Z",
     "iopub.status.idle": "2024-05-07T11:56:15.307131Z",
     "shell.execute_reply": "2024-05-07T11:56:15.305370Z",
     "shell.execute_reply.started": "2024-05-07T11:56:15.265691Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data via csv\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/'\n",
    "data = []\n",
    "for i in range(0, 6):\n",
    "    with open(f'links{i:03d}.csv') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    start = urlparse(df.iloc[0, 0]).path\n",
    "    dos = df.iloc[0, 1]\n",
    "    if not bool(urlparse(df.iloc[0, 0]).netloc):\n",
    "        path = df.iloc[:, 0].apply(lambda x: urljoin(url, x)).values.tolist()\n",
    "    else:\n",
    "        path = df.iloc[:, 0].values.tolist()\n",
    "    data.append([start, dos, path])\n",
    "n = 2\n",
    "df = pd.DataFrame(data, columns=['Start', 'DoS', 'Path'])\n",
    "df.to_csv(f'conso{n:02d}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055031dc-4b24-48eb-bf1f-6b03b962d323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
