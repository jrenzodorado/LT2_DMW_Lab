{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510f894c-02f9-4cb8-8b77-0d3dd52936f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd33b392-5d37-45bc-93eb-d701aeb080b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_parent(tag, pfr_list=True):\n",
    "    \"\"\"\n",
    "    Retrieves the parent tag of a given HTML tag.\n",
    "    It immediately returns the tag 'i' if found in the parent-finding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : bs4.element.Tag\n",
    "        The HTML tag whose parent is to be obtained.\n",
    "    pfr_list : bool, optional\n",
    "        If `True`, iterates through the primary tags (p, li, th, td) to find \n",
    "        the parent. If `False`, only returns the immediate parent.\n",
    "        Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parent : bs4.element.Tag or None\n",
    "        The parent tag of the input tag. Returns None if no parent is found.\n",
    "    \"\"\"\n",
    "\n",
    "    parent = tag.parent\n",
    "    if parent and parent.name == 'i':  # italics for exclusion later\n",
    "        return parent\n",
    "    parents = ['p', 'li', 'th', 'td']\n",
    "    if pfr_list:  # if not usual container, just use the first parent\n",
    "        while parent and parent.name not in parents:  # iterate until tag\n",
    "            parent = parent.parent\n",
    "            if parent and parent.name == 'i':  # italics for exclusion later\n",
    "                return parent\n",
    "    return parent\n",
    "\n",
    "\n",
    "def enclosed(tag, pfr_list=True):\n",
    "    \"\"\"\n",
    "    Checks if an HTML tag is enclosed within parentheses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : bs4.element.Tag\n",
    "        The HTML tag to be checked.\n",
    "    pfr_list : bool, optional\n",
    "        If `True`, considers the tag's immediate parent and its ancestors. If \n",
    "        `False`, only considers the immediate parent. Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the tag is enclosed within parentheses, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    parent = get_parent(tag, pfr_list)\n",
    "    if parent is None:\n",
    "        return False  # not enclosed if no parent\n",
    "    if parent.name == 'i':  # enclosed in italic tag\n",
    "        return True\n",
    "    parent_text = str(parent)  # includes the html tags\n",
    "    tag_text = str(tag)  # includes the html tags\n",
    "    idx = parent_text.index(tag_text)\n",
    "    l_cnt = parent_text[:idx].count('(')\n",
    "    r_cnt = parent_text[:idx].count(')')\n",
    "    if l_cnt == 0 or l_cnt - r_cnt != 1:  # left ( not found, or no open left\n",
    "        return False\n",
    "    l_cnt = parent_text[idx+len(tag_text):].count('(')\n",
    "    r_cnt = parent_text[idx+len(tag_text):].count(')')\n",
    "    # print('right >> ' + parent_text[idx+len(tag_text):])\n",
    "    if r_cnt == 0 or r_cnt - l_cnt != 1:  # right ( not found, or no open right\n",
    "        return False\n",
    "    return True  # enclosed\n",
    "\n",
    "\n",
    "def online(alink, url):\n",
    "    \"\"\"\n",
    "    Checks if an anchor link is online and accessible.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alink : bs4.element.Tag\n",
    "        The anchor link tag to be checked.\n",
    "    url : str\n",
    "        The base URL to which the anchor link is appended.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the anchor link is accessible, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(urljoin(url, alink['href']))\n",
    "    except Exception:\n",
    "        return False\n",
    "    if response.status_code == 200:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def wiki(alink, stop_words):\n",
    "    \"\"\"\n",
    "    Checks if an anchor link is a valid Wikipedia link.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alink : bs4.element.Tag\n",
    "        The anchor link tag to be checked.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the anchor link is a valid Wikipedia link, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    link = alink['href']\n",
    "    netloc = urlparse(link).netloc\n",
    "    if netloc == '' or 'wikipedia.org' in netloc:\n",
    "        return not any([word for word in stop_words if word in unquote(link)])\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_link(body, stop_words, url):\n",
    "    \"\"\"\n",
    "    Retrieves the first valid link within the primary tags (p, li, th, td)\n",
    "    of the main body.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body : bs4.element.Tag\n",
    "        The main content of the web page.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "    url : str\n",
    "        The base URL of the web page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link : str\n",
    "        The URL of the first valid link found.\n",
    "    \"\"\"\n",
    "\n",
    "    link = ''\n",
    "    a_str = ('a[href]:not([href^=\"#cite_note\"]):not([href^=\"#\"])'\n",
    "             ':not(.mw-disambig)')\n",
    "    p_str = ['p', 'li', 'th', 'td']\n",
    "    alinks = body.select(', '.join([f'{p} {a_str} ' for p in p_str]))\n",
    "    for alink in alinks:\n",
    "        if (wiki(alink, stop_words) and online(alink, url)\n",
    "                and not enclosed(alink)):\n",
    "            link = alink['href']\n",
    "            break\n",
    "    return link\n",
    "\n",
    "\n",
    "def get_other_link(body, extags, stop_words, url):\n",
    "    \"\"\"\n",
    "    Retrieves a valid link from the main body if no valid links are found\n",
    "    in get_link().\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body : bs4.element.Tag\n",
    "        The main content of the web page.\n",
    "    extags : list\n",
    "        List of extracted tags from the main content to be added back.\n",
    "    stop_words : list\n",
    "        List of stop words to exclude invalid links.\n",
    "    url : str\n",
    "        The base URL of the web page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link : str\n",
    "        The URL of another valid link found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    IndexError\n",
    "        If no valid links are found.\n",
    "    \"\"\"\n",
    "\n",
    "    link = ''\n",
    "    a_str = ('a[href]:not([href^=\"#cite_note\"]):not([href^=\"#\"]):'\n",
    "             'not(.mw-disambig)')\n",
    "    for extag in extags:  # add back extracted tags for full search\n",
    "        body.append(extag)\n",
    "    alinks = body.select(f'{a_str}')\n",
    "    for alink in alinks:\n",
    "        if (wiki(alink, stop_words) and online(alink, url)\n",
    "                and not enclosed(alink, False)):\n",
    "            link = alink['href']\n",
    "            break\n",
    "    if link == '':  # no valid links in the body\n",
    "        raise IndexError\n",
    "    return link\n",
    "\n",
    "def save_linkhist(linkhist, dir_, a, iter, fin=False):\n",
    "    \"\"\"\n",
    "    Saves the Link:DoS history to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    linkhist : dict\n",
    "        Dictionary containing link:DoS mappings.\n",
    "    dir_ : str\n",
    "        Directory where the CSV file will be saved.\n",
    "    a : int\n",
    "        Starting index used in the filename for identification.\n",
    "    iter : int\n",
    "        Iteration number used in the filename for identification.\n",
    "    fin : bool, optional\n",
    "        If `True`, indicates the final save. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    dflhist = pd.DataFrame(list(linkhist.items()), columns=['Link', 'DoS'])\n",
    "    buname = f'_bu{a:03d}.{iter:03d}' if not fin else ''\n",
    "    dflhist.to_csv(f'{dir_}linkhist{buname}.csv', index=False)\n",
    "    print(f'Link:DoS {\"backup \" if not fin else \"\"}history saved to {dir_}linkhist{buname}.csv')\n",
    "    return None\n",
    "\n",
    "def save_conso(data, dir_, a, iter, fin=False):\n",
    "    \"\"\"\n",
    "    Saves consolidated crawl results to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        List containing crawl data entries.\n",
    "    dir_ : str\n",
    "        Directory where the CSV file will be saved.\n",
    "    a : int\n",
    "        Starting index used in the filename for identification.\n",
    "    iter : int\n",
    "        Iteration number used in the filename for identification.\n",
    "    fin : bool, optional\n",
    "        If `True`, indicates the final save. Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or None\n",
    "        Returns a DataFrame if `fin` is True; otherwise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Start', 'DoS', 'Path', 'Msg'])\n",
    "    buname = '_bu' if not fin else ''\n",
    "    df.to_csv(f'{dir_}conso{buname}{a:03d}.{iter:03d}.csv', index=False)\n",
    "    print(f'Consolidated {\"backup \" if not fin else \"\"}results saved to '\n",
    "          f'{dir_}conso{buname}{a:03d}.{iter:03d}.csv')\n",
    "    return df if fin else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2afe29b8-3d40-4e8e-95a9-2b7455d29d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def web_crawler(a, dir_, url=None, lang='en', linkhist={}):\n",
    "    \"\"\"\n",
    "    This function starts from a random Wikipedia page and follows the first\n",
    "    page link until it reaches the Philosophy page, a page with no links, or\n",
    "    loops back to a previously visited link.\n",
    "    This function accepts a link:DoS (linkhist) dictionary which shorts the\n",
    "    path to Philosopy, removing redundant crawls from previous runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : int\n",
    "        Index for the web crawl.\n",
    "    dir_ : str\n",
    "        Directory path where the output scrape CSV file will be saved.\n",
    "    url : str, optional\n",
    "        The starting URL for the web crawl. If not provided, a random Wikipedia\n",
    "        page URL will be chosen based on the specified language.\n",
    "    lang : str, optional\n",
    "        The language code for the Wikipedia pages. Defaults to 'en' (English).\n",
    "        Other options: 'es' (Spanish), 'fr' (French), 'de' (German).\n",
    "    linkhist : dict, optional\n",
    "        Dictionary to store link history - used for the short path, reducing\n",
    "        crawl iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    flinks : list\n",
    "        List of visited page URLs during the crawl.\n",
    "    dosl : list\n",
    "        List of degrees of separation (DoS) for each visited page.\n",
    "    msg : str\n",
    "        Message indicating the status of the crawl (e.g., 'OK - normal path' or\n",
    "        'NOK - no links found').\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function excludes certain types of links (e.g., language links,\n",
    "      disambiguation pages, external links) during the crawl.\n",
    "    - It prioritizes links within specific HTML tags and considers the main tex\n",
    "      of the Wikipedia page.\n",
    "    - The DoS represents the number of steps from the starting page to the\n",
    "      Philosophy page.\n",
    "    - It outputs the single scrape result into a CSV file.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    flinks, dosl, msg = web_crawler(1, 'output/', lang='en')\n",
    "    \"\"\"\n",
    "\n",
    "    if not bool(url):\n",
    "        url = f'https://{lang}.wikipedia.org/wiki/Special:Random'\n",
    "    philo = {\n",
    "        'en': 'Philosophy',\n",
    "        'de': 'Philosophie',\n",
    "        'es': 'Psicología',\n",
    "        'fr': 'Philosophie'\n",
    "    }\n",
    "    target = f'https://{lang}.wikipedia.org/wiki/{philo[lang]}'\n",
    "    stop_words = {\n",
    "        'en': ['Wikipedia:', 'Wikipedia talk:', 'User:', 'User_talk:', 'Talk:',\n",
    "               'Help:', 'Help talk:', 'Project:', 'Project talk:',\n",
    "               'Portal:', 'Portal talk:', 'Template:', 'Template talk:',\n",
    "               'File:', 'File talk:', 'Special:', 'index.php',\n",
    "               'Category:', 'Category talk:', 'Template_talk:', 'MOS:', 'Special:'],\n",
    "        'de': ['Wikipedia:', 'Wikipedia Diskussion:', 'Benutzer:', 'Benutzer Diskussion:',\n",
    "               'Diskussion:',\n",
    "               'Hilfe:', 'Hilfe Diskussion:', 'Projekt:',\n",
    "               'Projekt Diskussion:',\n",
    "               'Portal:', 'Portal Diskussion:', 'Vorlage:',\n",
    "               'Vorlage Diskussion:',\n",
    "               'Datei:', 'Datei Diskussion:', 'Spezial:', 'index.php',\n",
    "               'Kategorie:', 'Kategorie Diskussion:', 'Vorlage Diskussion:',\n",
    "               'MOS:', 'Special:'],\n",
    "        'es': ['Wikipedia:', 'Wikipedia discusión:', 'Usuario:', 'Usuario discusión:',\n",
    "               'Discusión:',\n",
    "               'Ayuda:', 'Ayuda discusión:', 'Wikiproyecto:', 'Wikiproyecto discusión:',\n",
    "               'Portal:', 'Portal discusión:', 'Plantilla:', 'Plantilla discusión:',\n",
    "               'Archivo:', 'Archivo discusión:', 'Especial:', 'index.php',\n",
    "               'Categoría:', 'Categoría discusión:', 'Plantilla discusión:', 'WP:',\n",
    "               'Special:'],\n",
    "        'fr': ['Wikipédia:', 'Discussion Wikipédia:', 'Utilisateur:',\n",
    "               'Discussion utilisateur:', 'Discussion:', 'Aide:', 'Discussion aide:',\n",
    "               'Projet:', 'Discussion Projet:', 'Portail:', 'Discussion Portail:',\n",
    "               'Modèle:', 'Discussion modèle:', 'Fichier:', 'Discussion fichier:',\n",
    "               'Spécial:', 'index.php', 'Catégorie:', 'Discussion catégorie:',\n",
    "               'Discussion modèle:', 'MOS:', 'Spécial:']\n",
    "\n",
    "    }\n",
    "    s = requests.Session()\n",
    "    response = s.get(url)\n",
    "    link = urlparse(response.url).path\n",
    "    flink = urljoin(url, link)\n",
    "    flinks = []\n",
    "    flinks.append(flink)\n",
    "    print(link)\n",
    "    ldos = 0\n",
    "    dos = -1\n",
    "    msg = ''\n",
    "    while True:\n",
    "        response = s.get(flink)\n",
    "        soup = bs4.BeautifulSoup(response.text)\n",
    "        body = soup.find('div', id='bodyContent')\n",
    "        for table in body.find_all('table', class_=['infobox', 'sidebar',\n",
    "                                                    'metadata']):\n",
    "            table.extract()\n",
    "        for div in body.find_all('div', class_=['infobox', 'sidebar',\n",
    "                                                'metadata']):\n",
    "            div.extract()\n",
    "        for div in body.find_all('div', role='note'):\n",
    "            div.extract()\n",
    "        extags = []\n",
    "        for extag in body.find_all('div', class_='thumbcaption'):\n",
    "            extags.append(extag.extract())\n",
    "        for extag in body.find_all('figcaption'):\n",
    "            extags.append(extag.extract())\n",
    "        for extag in body.find_all('table', class_=['infobox',\n",
    "                                                    'standings-box']):\n",
    "            extags.append(extag.extract())\n",
    "        try:\n",
    "            link = get_link(body, stop_words[lang], url)\n",
    "            if link == '':  # no valid link within p, li, th, or td tags\n",
    "                link = get_other_link(body, extags, stop_words[lang], url)\n",
    "            flink = urljoin(url, link)\n",
    "        except IndexError:\n",
    "            print('No valid links found in current page.')\n",
    "            print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "            msg = f'NOK - no links at {link} (last idx: {len(flinks)-1})'\n",
    "            break\n",
    "        # link = link.split('wiki/')[-1]\n",
    "        if unquote(flink) == target:\n",
    "            flinks.append(flink)\n",
    "            print(link)\n",
    "            print(f'Successfully reached {philo[lang]} w/ DoS '\n",
    "                  f'= {len(flinks)-1}.')\n",
    "            msg = 'OK - normal path.'\n",
    "            break\n",
    "        elif flink in flinks:\n",
    "            print(f'Looped back to {link}.')\n",
    "            print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "            msg = (f'NOK - looped at {link} (link idx: {flinks.index(flink)}; '\n",
    "                   f'last idx: {len(flinks)-1})')\n",
    "            break\n",
    "        else:\n",
    "            flinks.append(flink)\n",
    "            print(link)\n",
    "            if link in linkhist:\n",
    "                ldos = linkhist[link]\n",
    "                print(f'Short path found ({link}; DoS = {ldos}).')\n",
    "                dos = -1 if ldos == -1 else len(flinks) - 1 + ldos\n",
    "                if dos == -1:\n",
    "                    print(f'Failed to reach {philo[lang]}; DoS = -1.')\n",
    "                    msg = (f'NOK - short path at {link} '\n",
    "                           f'(last idx: {len(flinks)-1}).')\n",
    "                else:\n",
    "                    print(f'Successfully reached {philo[lang]} '\n",
    "                          f'w/ DoS = {dos}.')\n",
    "                    msg = (f'OK - short path at {link} '\n",
    "                           f'(last idx: {len(flinks)-1}).')\n",
    "                break\n",
    "        time.sleep(1.25)\n",
    "\n",
    "    print(flinks)\n",
    "    if unquote(flinks[-1]) == urljoin(url, target):\n",
    "        dos = len(flinks) - 1\n",
    "    if dos == -1:\n",
    "        dosl = -1*np.ones(len(flinks))\n",
    "    else:\n",
    "        dosl = np.arange(dos, ldos-1, -1)\n",
    "    df = pd.DataFrame({'Links': flinks, 'DoS': dosl})\n",
    "    if not os.path.exists(f'{dir_}'):\n",
    "        os.makedirs(f'{dir_}')\n",
    "    df.to_csv(f'{dir_}links{a:03d}.csv', index=False)\n",
    "    print(f'Crawl result saved to {dir_}links{a:03d}.csv')\n",
    "    return flinks, dosl, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f23ad7aa-3575-4b6d-951f-fee000094aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling Spanish Wikipedia...\n",
      "Link:DoS history not loaded - short paths will occur more frequency with more iterations.\n",
      "\n",
      "--- Iter 0 ---\n",
      "/wiki/Liu_Xiuhua\n",
      "/wiki/Rep%C3%BAblica_Popular_China\n",
      "/wiki/Estado_soberano\n",
      "/wiki/Derecho_internacional\n",
      "/wiki/Estado\n",
      "/wiki/Instituciones\n",
      "/wiki/Relaciones_sociales\n",
      "/wiki/Interacci%C3%B3n_social\n",
      "/wiki/Conflicto\n",
      "/wiki/Poder_(social_y_pol%C3%ADtico)\n",
      "/wiki/Ciencias_sociales\n",
      "/wiki/Ramas_de_la_ciencia\n",
      "/wiki/Ciencias_formales\n",
      "/wiki/Ciencia\n",
      "/wiki/Sistema\n",
      "/wiki/Concepto\n",
      "/wiki/Entendimiento\n",
      "/wiki/Aptitud\n",
      "/wiki/Psicolog%C3%ADa\n",
      "Successfully reached Psicología w/ DoS = 18.\n",
      "['https://es.wikipedia.org/wiki/Liu_Xiuhua', 'https://es.wikipedia.org/wiki/Rep%C3%BAblica_Popular_China', 'https://es.wikipedia.org/wiki/Estado_soberano', 'https://es.wikipedia.org/wiki/Derecho_internacional', 'https://es.wikipedia.org/wiki/Estado', 'https://es.wikipedia.org/wiki/Instituciones', 'https://es.wikipedia.org/wiki/Relaciones_sociales', 'https://es.wikipedia.org/wiki/Interacci%C3%B3n_social', 'https://es.wikipedia.org/wiki/Conflicto', 'https://es.wikipedia.org/wiki/Poder_(social_y_pol%C3%ADtico)', 'https://es.wikipedia.org/wiki/Ciencias_sociales', 'https://es.wikipedia.org/wiki/Ramas_de_la_ciencia', 'https://es.wikipedia.org/wiki/Ciencias_formales', 'https://es.wikipedia.org/wiki/Ciencia', 'https://es.wikipedia.org/wiki/Sistema', 'https://es.wikipedia.org/wiki/Concepto', 'https://es.wikipedia.org/wiki/Entendimiento', 'https://es.wikipedia.org/wiki/Aptitud', 'https://es.wikipedia.org/wiki/Psicolog%C3%ADa']\n",
      "Crawl result saved to es_res0515b/links000.csv\n",
      "\n",
      "--- Iter 1 ---\n",
      "/wiki/Gornji_Hrgovi\n",
      "/wiki/Pueblo_(poblaci%C3%B3n_rural)\n",
      "/wiki/Localidad\n",
      "/wiki/N%C3%BAcleo_de_poblaci%C3%B3n\n",
      "/wiki/Asentamiento\n",
      "/wiki/Persona\n",
      "/wiki/Individuo\n",
      "/wiki/Uno\n",
      "/wiki/N%C3%BAmero_natural\n",
      "/wiki/Matem%C3%A1ticas\n",
      "/wiki/Ciencias_formales\n",
      "Short path found (/wiki/Ciencias_formales; DoS = 6).\n",
      "Successfully reached Psicología w/ DoS = 16.\n",
      "['https://es.wikipedia.org/wiki/Gornji_Hrgovi', 'https://es.wikipedia.org/wiki/Pueblo_(poblaci%C3%B3n_rural)', 'https://es.wikipedia.org/wiki/Localidad', 'https://es.wikipedia.org/wiki/N%C3%BAcleo_de_poblaci%C3%B3n', 'https://es.wikipedia.org/wiki/Asentamiento', 'https://es.wikipedia.org/wiki/Persona', 'https://es.wikipedia.org/wiki/Individuo', 'https://es.wikipedia.org/wiki/Uno', 'https://es.wikipedia.org/wiki/N%C3%BAmero_natural', 'https://es.wikipedia.org/wiki/Matem%C3%A1ticas', 'https://es.wikipedia.org/wiki/Ciencias_formales']\n",
      "Crawl result saved to es_res0515b/links001.csv\n",
      "\n",
      "--- Iter 2 ---\n",
      "/wiki/Isla_Jidda\n",
      "/wiki/Isla_Iris\n",
      "/wiki/Mar_Mediterr%C3%A1neo\n",
      "/wiki/Mar\n",
      "/wiki/Oc%C3%A9ano\n",
      "/wiki/Hidr%C3%B3sfera\n",
      "/wiki/Ciencias_de_la_Tierra\n",
      "/wiki/Geolog%C3%ADa\n",
      "/wiki/Ciencias_naturales\n",
      "/wiki/Ciencia\n",
      "Short path found (/wiki/Ciencia; DoS = 5).\n",
      "Successfully reached Psicología w/ DoS = 14.\n",
      "['https://es.wikipedia.org/wiki/Isla_Jidda', 'https://es.wikipedia.org/wiki/Isla_Iris', 'https://es.wikipedia.org/wiki/Mar_Mediterr%C3%A1neo', 'https://es.wikipedia.org/wiki/Mar', 'https://es.wikipedia.org/wiki/Oc%C3%A9ano', 'https://es.wikipedia.org/wiki/Hidr%C3%B3sfera', 'https://es.wikipedia.org/wiki/Ciencias_de_la_Tierra', 'https://es.wikipedia.org/wiki/Geolog%C3%ADa', 'https://es.wikipedia.org/wiki/Ciencias_naturales', 'https://es.wikipedia.org/wiki/Ciencia']\n",
      "Crawl result saved to es_res0515b/links002.csv\n",
      "Link:DoS backup history saved to es_res0515b/linkhist_bu000.002.csv\n",
      "Consolidated backup results saved to es_res0515b/conso_bu000.002.csv\n",
      "\n",
      "--- Iter 3 ---\n",
      "/wiki/Zawiesiuchy\n",
      "/wiki/Alfabeto_Fon%C3%A9tico_Internacional\n",
      "/wiki/Transcripci%C3%B3n_fon%C3%A9tica\n",
      "/wiki/Habla\n",
      "/wiki/Lengua_(ling%C3%BC%C3%ADstica)\n",
      "/wiki/Comunicaci%C3%B3n\n",
      "/wiki/Conciencia\n",
      "/wiki/Conocimiento\n",
      "/wiki/Hecho_(filosof%C3%ADa)\n",
      "/wiki/Lexema\n",
      "/wiki/Morfema\n",
      "/wiki/Monema\n",
      "Looped back to /wiki/Morfema.\n",
      "Failed to reach Psicología; DoS = -1.\n",
      "['https://es.wikipedia.org/wiki/Zawiesiuchy', 'https://es.wikipedia.org/wiki/Alfabeto_Fon%C3%A9tico_Internacional', 'https://es.wikipedia.org/wiki/Transcripci%C3%B3n_fon%C3%A9tica', 'https://es.wikipedia.org/wiki/Habla', 'https://es.wikipedia.org/wiki/Lengua_(ling%C3%BC%C3%ADstica)', 'https://es.wikipedia.org/wiki/Comunicaci%C3%B3n', 'https://es.wikipedia.org/wiki/Conciencia', 'https://es.wikipedia.org/wiki/Conocimiento', 'https://es.wikipedia.org/wiki/Hecho_(filosof%C3%ADa)', 'https://es.wikipedia.org/wiki/Lexema', 'https://es.wikipedia.org/wiki/Morfema', 'https://es.wikipedia.org/wiki/Monema']\n",
      "Crawl result saved to es_res0515b/links003.csv\n",
      "\n",
      "--- Iter 4 ---\n",
      "/wiki/Pitis_de_Palembang\n",
      "/wiki/Sultanato_de_Palembang\n",
      "/wiki/Palembang\n",
      "/wiki/Provincia_de_Sumatra_Meridional\n",
      "/wiki/Provincias_de_Indonesia\n",
      "/wiki/Gobernador\n",
      "/wiki/Pol%C3%ADtico\n",
      "/wiki/Pol%C3%ADtica\n",
      "/wiki/Toma_de_decisiones\n",
      "/wiki/Vida\n",
      "/wiki/Sistema\n",
      "Short path found (/wiki/Sistema; DoS = 4).\n",
      "Successfully reached Psicología w/ DoS = 14.\n",
      "['https://es.wikipedia.org/wiki/Pitis_de_Palembang', 'https://es.wikipedia.org/wiki/Sultanato_de_Palembang', 'https://es.wikipedia.org/wiki/Palembang', 'https://es.wikipedia.org/wiki/Provincia_de_Sumatra_Meridional', 'https://es.wikipedia.org/wiki/Provincias_de_Indonesia', 'https://es.wikipedia.org/wiki/Gobernador', 'https://es.wikipedia.org/wiki/Pol%C3%ADtico', 'https://es.wikipedia.org/wiki/Pol%C3%ADtica', 'https://es.wikipedia.org/wiki/Toma_de_decisiones', 'https://es.wikipedia.org/wiki/Vida', 'https://es.wikipedia.org/wiki/Sistema']\n",
      "Crawl result saved to es_res0515b/links004.csv\n",
      "Link:DoS backup history saved to es_res0515b/linkhist_bu000.004.csv\n",
      "Consolidated backup results saved to es_res0515b/conso_bu000.004.csv\n",
      "\n",
      "--- Iter 5 ---\n",
      "/wiki/Gran_Premio_de_Jasna_G%C3%B3ra\n",
      "/wiki/Jasna_G%C3%B3ra\n",
      "/wiki/Czestochowa\n",
      "/wiki/Polonia\n",
      "/wiki/Europa_Central\n",
      "/wiki/Europa\n",
      "/wiki/Continente\n",
      "/wiki/Territorio\n",
      "/wiki/%C3%81rea\n",
      "/wiki/Espacio_m%C3%A9trico\n",
      "/wiki/Matem%C3%A1tica\n",
      "/wiki/Ciencias_formales\n",
      "Short path found (/wiki/Ciencias_formales; DoS = 6).\n",
      "Successfully reached Psicología w/ DoS = 17.\n",
      "['https://es.wikipedia.org/wiki/Gran_Premio_de_Jasna_G%C3%B3ra', 'https://es.wikipedia.org/wiki/Jasna_G%C3%B3ra', 'https://es.wikipedia.org/wiki/Czestochowa', 'https://es.wikipedia.org/wiki/Polonia', 'https://es.wikipedia.org/wiki/Europa_Central', 'https://es.wikipedia.org/wiki/Europa', 'https://es.wikipedia.org/wiki/Continente', 'https://es.wikipedia.org/wiki/Territorio', 'https://es.wikipedia.org/wiki/%C3%81rea', 'https://es.wikipedia.org/wiki/Espacio_m%C3%A9trico', 'https://es.wikipedia.org/wiki/Matem%C3%A1tica', 'https://es.wikipedia.org/wiki/Ciencias_formales']\n",
      "Crawl result saved to es_res0515b/links005.csv\n",
      "\n",
      "--- Iter 6 ---\n",
      "/wiki/Campeonato_Mundial_de_Lucha_de_1922\n",
      "/wiki/Campeonato_Mundial_de_Lucha\n",
      "/wiki/Lucha_(deporte)\n",
      "/wiki/Atletismo\n",
      "/wiki/Deporte\n",
      "/wiki/Juego\n",
      "/wiki/Actividad\n",
      "/wiki/Psicolog%C3%ADa\n",
      "Successfully reached Psicología w/ DoS = 7.\n",
      "['https://es.wikipedia.org/wiki/Campeonato_Mundial_de_Lucha_de_1922', 'https://es.wikipedia.org/wiki/Campeonato_Mundial_de_Lucha', 'https://es.wikipedia.org/wiki/Lucha_(deporte)', 'https://es.wikipedia.org/wiki/Atletismo', 'https://es.wikipedia.org/wiki/Deporte', 'https://es.wikipedia.org/wiki/Juego', 'https://es.wikipedia.org/wiki/Actividad', 'https://es.wikipedia.org/wiki/Psicolog%C3%ADa']\n",
      "Crawl result saved to es_res0515b/links006.csv\n",
      "Link:DoS backup history saved to es_res0515b/linkhist_bu000.006.csv\n",
      "Consolidated backup results saved to es_res0515b/conso_bu000.006.csv\n",
      "\n",
      "--- Iter 7 ---\n",
      "/wiki/Peter_J._Denning\n",
      "/wiki/Inform%C3%A1tico_te%C3%B3rico\n",
      "/wiki/Cient%C3%ADfico_de_la_computaci%C3%B3n\n",
      "Looped back to /wiki/Cient%C3%ADfico_de_la_computaci%C3%B3n.\n",
      "Failed to reach Psicología; DoS = -1.\n",
      "['https://es.wikipedia.org/wiki/Peter_J._Denning', 'https://es.wikipedia.org/wiki/Inform%C3%A1tico_te%C3%B3rico', 'https://es.wikipedia.org/wiki/Cient%C3%ADfico_de_la_computaci%C3%B3n']\n",
      "Crawl result saved to es_res0515b/links007.csv\n",
      "\n",
      "--- Iter 8 ---\n",
      "/wiki/Mateo_Santamarta\n",
      "/wiki/Provincia_de_Le%C3%B3n\n",
      "/wiki/Provincia_de_Espa%C3%B1a\n",
      "/wiki/Demarcaci%C3%B3n_administrativa\n",
      "/wiki/Entidades_pol%C3%ADticas\n",
      "/wiki/Ciencia_pol%C3%ADtica\n",
      "/wiki/Ciencia_social\n",
      "/wiki/Ramas_de_la_ciencia\n",
      "Short path found (/wiki/Ramas_de_la_ciencia; DoS = 7).\n",
      "Successfully reached Psicología w/ DoS = 14.\n",
      "['https://es.wikipedia.org/wiki/Mateo_Santamarta', 'https://es.wikipedia.org/wiki/Provincia_de_Le%C3%B3n', 'https://es.wikipedia.org/wiki/Provincia_de_Espa%C3%B1a', 'https://es.wikipedia.org/wiki/Demarcaci%C3%B3n_administrativa', 'https://es.wikipedia.org/wiki/Entidades_pol%C3%ADticas', 'https://es.wikipedia.org/wiki/Ciencia_pol%C3%ADtica', 'https://es.wikipedia.org/wiki/Ciencia_social', 'https://es.wikipedia.org/wiki/Ramas_de_la_ciencia']\n",
      "Crawl result saved to es_res0515b/links008.csv\n",
      "Link:DoS backup history saved to es_res0515b/linkhist_bu000.008.csv\n",
      "Consolidated backup results saved to es_res0515b/conso_bu000.008.csv\n",
      "\n",
      "--- Iter 9 ---\n",
      "/wiki/Anexo:XIX_Premios_Iris\n",
      "/wiki/Premios_Iris_de_Espa%C3%B1a\n",
      "/wiki/Academia_de_Televisi%C3%B3n_y_de_las_Ciencias_y_Artes_del_Audiovisual_de_Espa%C3%B1a\n",
      "/wiki/Premios_ATV\n",
      "Looped back to /wiki/Academia_de_Televisi%C3%B3n_y_de_las_Ciencias_y_Artes_del_Audiovisual_de_Espa%C3%B1a.\n",
      "Failed to reach Psicología; DoS = -1.\n",
      "['https://es.wikipedia.org/wiki/Anexo:XIX_Premios_Iris', 'https://es.wikipedia.org/wiki/Premios_Iris_de_Espa%C3%B1a', 'https://es.wikipedia.org/wiki/Academia_de_Televisi%C3%B3n_y_de_las_Ciencias_y_Artes_del_Audiovisual_de_Espa%C3%B1a', 'https://es.wikipedia.org/wiki/Premios_ATV']\n",
      "Crawl result saved to es_res0515b/links009.csv\n",
      "Consolidated results saved to es_res0515b/conso000.009.csv\n",
      "Link:DoS history saved to es_res0515b/linkhist.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>DoS</th>\n",
       "      <th>Path</th>\n",
       "      <th>Msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/wiki/Liu_Xiuhua</td>\n",
       "      <td>18.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Liu_Xiuhua, htt...</td>\n",
       "      <td>OK - normal path.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/wiki/Gornji_Hrgovi</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Gornji_Hrgovi, ...</td>\n",
       "      <td>OK - short path at /wiki/Ciencias_formales (la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/wiki/Isla_Jidda</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Isla_Jidda, htt...</td>\n",
       "      <td>OK - short path at /wiki/Ciencia (last idx: 9).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/wiki/Zawiesiuchy</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Zawiesiuchy, ht...</td>\n",
       "      <td>NOK - looped at /wiki/Morfema (link idx: 10; l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/wiki/Pitis_de_Palembang</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Pitis_de_Palemb...</td>\n",
       "      <td>OK - short path at /wiki/Sistema (last idx: 10).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/wiki/Gran_Premio_de_Jasna_G%C3%B3ra</td>\n",
       "      <td>17.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Gran_Premio_de_...</td>\n",
       "      <td>OK - short path at /wiki/Ciencias_formales (la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/wiki/Campeonato_Mundial_de_Lucha_de_1922</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Campeonato_Mund...</td>\n",
       "      <td>OK - normal path.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/wiki/Peter_J._Denning</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Peter_J._Dennin...</td>\n",
       "      <td>NOK - looped at /wiki/Cient%C3%ADfico_de_la_co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/wiki/Mateo_Santamarta</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Mateo_Santamart...</td>\n",
       "      <td>OK - short path at /wiki/Ramas_de_la_ciencia (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/wiki/Anexo:XIX_Premios_Iris</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[https://es.wikipedia.org/wiki/Anexo:XIX_Premi...</td>\n",
       "      <td>NOK - looped at /wiki/Academia_de_Televisi%C3%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Start   DoS  \\\n",
       "0                           /wiki/Liu_Xiuhua  18.0   \n",
       "1                        /wiki/Gornji_Hrgovi  16.0   \n",
       "2                           /wiki/Isla_Jidda  14.0   \n",
       "3                          /wiki/Zawiesiuchy  -1.0   \n",
       "4                   /wiki/Pitis_de_Palembang  14.0   \n",
       "5       /wiki/Gran_Premio_de_Jasna_G%C3%B3ra  17.0   \n",
       "6  /wiki/Campeonato_Mundial_de_Lucha_de_1922   7.0   \n",
       "7                     /wiki/Peter_J._Denning  -1.0   \n",
       "8                     /wiki/Mateo_Santamarta  14.0   \n",
       "9               /wiki/Anexo:XIX_Premios_Iris  -1.0   \n",
       "\n",
       "                                                Path  \\\n",
       "0  [https://es.wikipedia.org/wiki/Liu_Xiuhua, htt...   \n",
       "1  [https://es.wikipedia.org/wiki/Gornji_Hrgovi, ...   \n",
       "2  [https://es.wikipedia.org/wiki/Isla_Jidda, htt...   \n",
       "3  [https://es.wikipedia.org/wiki/Zawiesiuchy, ht...   \n",
       "4  [https://es.wikipedia.org/wiki/Pitis_de_Palemb...   \n",
       "5  [https://es.wikipedia.org/wiki/Gran_Premio_de_...   \n",
       "6  [https://es.wikipedia.org/wiki/Campeonato_Mund...   \n",
       "7  [https://es.wikipedia.org/wiki/Peter_J._Dennin...   \n",
       "8  [https://es.wikipedia.org/wiki/Mateo_Santamart...   \n",
       "9  [https://es.wikipedia.org/wiki/Anexo:XIX_Premi...   \n",
       "\n",
       "                                                 Msg  \n",
       "0                                  OK - normal path.  \n",
       "1  OK - short path at /wiki/Ciencias_formales (la...  \n",
       "2    OK - short path at /wiki/Ciencia (last idx: 9).  \n",
       "3  NOK - looped at /wiki/Morfema (link idx: 10; l...  \n",
       "4   OK - short path at /wiki/Sistema (last idx: 10).  \n",
       "5  OK - short path at /wiki/Ciencias_formales (la...  \n",
       "6                                  OK - normal path.  \n",
       "7  NOK - looped at /wiki/Cient%C3%ADfico_de_la_co...  \n",
       "8  OK - short path at /wiki/Ramas_de_la_ciencia (...  \n",
       "9  NOK - looped at /wiki/Academia_de_Televisi%C3%...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main function to generate data (can be random, or can be from a list of urls)\n",
    "\n",
    "def crawl_iter(basedir, urls=list(), lang='en', a=0, b=100, test=False):\n",
    "    \"\"\"\n",
    "    This function iteratively calls the web_crawler function.\n",
    "    It also loads and saves a link:DoS (linkhist) dictionary which shorts\n",
    "    the path to Philosopy in web_crawler, removing redundant crawls from\n",
    "    previous runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    basedir : str\n",
    "        Base directory where output files will be saved.\n",
    "    urls : list, optional\n",
    "        List of specific URLs to crawl. If not provided, random Wikipedia pages \n",
    "        will be crawled.\n",
    "    lang : str, optional\n",
    "        Language code for Wikipedia pages. Defaults to 'en' (English).\n",
    "        Other options: 'es' (Spanish), 'fr' (French), 'de' (German).\n",
    "    a : int, optional\n",
    "        Starting index for iteration. Defaults to 0.\n",
    "    b : int, optional\n",
    "        Ending index for iteration. Defaults to 100.\n",
    "        This index is ignored if 'urls' is passed.\n",
    "    test : bool, optional\n",
    "        If `True`, performs a test run without saving Link:DoS history.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing consolidated results of the crawl.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function saves Link:DoS history in a CSV file for future iterations.\n",
    "    - It consolidates crawl results into a CSV file based on the specified \n",
    "      iteration range.\n",
    "    - Specify `test=True` for a trial run without saving Link:DoS history.\n",
    "    - Any comparison to target and stopwords need to be unquoted.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    df = crawl_iter('output/', lang='en', a=0, b=50)\n",
    "    \"\"\"\n",
    "    langs = {'en': 'English', 'de': 'German', 'es': 'Spanish', 'fr': 'French'}\n",
    "    dir_ = f'{lang}_{basedir}'\n",
    "    print(f'Crawling {langs[lang]} Wikipedia...')\n",
    "    try:\n",
    "        dflhist = pd.read_csv(f'{dir_}linkhist.csv', header=0)\n",
    "        linkhist = dict(zip(dflhist['Link'], dflhist['DoS']))\n",
    "        print(f'Link:DoS history loaded from {dir_}linkhist.csv - short paths '\n",
    "              'from history will be used.')\n",
    "        print(f'History will{\"\" if not test else \" NOT\"} be updated after this '\n",
    "              'run.')\n",
    "        # print(linkhist)\n",
    "    except Exception:\n",
    "        # print(e)\n",
    "        print('Link:DoS history not loaded - short paths will occur more '\n",
    "              'frequency with more iterations.')\n",
    "        linkhist = dict()\n",
    "    if bool(urls):\n",
    "        b = a + len(urls)\n",
    "    data = []\n",
    "    for i in range(a, b):\n",
    "        url = urls[i - a] if bool(urls) else ''\n",
    "        print(f'\\n--- Iter {i} ---')\n",
    "        flinks, dosl, msg = web_crawler(i, dir_, url, lang, linkhist)\n",
    "        data.append([urlparse(flinks[0]).path, dosl[0], flinks, msg])\n",
    "        links = [urlparse(flink).path for flink in flinks]\n",
    "        linkhist.update({link: dos.astype(int) for link, dos in zip(links, dosl)})\n",
    "        if (i-a) % 2 == 0 and i-a != 0:\n",
    "            save_conso(data, dir_, a, i)\n",
    "            save_linkhist(linkhist, dir_, a, i)\n",
    "    df = save_conso(data, dir_, a, b-1, fin=True)\n",
    "    # print(linkhist)\n",
    "    if not test:\n",
    "        save_linkhist(linkhist, dir_, a, b-1, fin=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# urls = ['https://en.wikipedia.org/wiki/Kevin_Bacon',\n",
    "#         'https://en.wikipedia.org/wiki/Doom_(2016_video_game)']\n",
    "# urls = ['https://en.wikipedia.org/wiki/wiki/Baarbach',\n",
    "#         'https://en.wikipedia.org/wiki/wiki/Data']\n",
    "# urls = ['https://es.wikipedia.org/wiki/Territorio']\n",
    "# urls = ['https://es.wikipedia.org/wiki/Arte']\n",
    "urls = []\n",
    "crawl_iter(basedir='res0515b/', urls=urls, lang='es', a=0, b=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a15ab-2b0f-403d-8e1f-1ee879ea32fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# diff data\n",
    "\n",
    "basedir = 'res0513_val/'\n",
    "lang = 'en'\n",
    "url = f'https://{lang}.wikipedia.org/'\n",
    "in_path = 'wiki_scraper_results_val.csv'\n",
    "data = []\n",
    "with open(in_path, 'r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "a, b = 11, 100  # start and end index to retrieve\n",
    "df = df.iloc[a: b].reset_index(drop=True)\n",
    "display(df)\n",
    "urls = df['article'].apply(lambda x: urljoin(url, x)).values.tolist()\n",
    "ndf = crawl_iter(basedir=basedir, urls=urls, a=a, b=b)\n",
    "display(ndf)\n",
    "df['nDoS'] = ndf['DoS']\n",
    "df['nPath'] = ndf['Path']\n",
    "df['diffDoS'] = df['nDoS'] - df['DoS']\n",
    "df['diffPath'] = df['nPath'] == df['path']\n",
    "df.to_csv(f'{lang}_{basedir}diff{a:03d}.{b-1:03d}.csv', index=False)\n",
    "print(f'Diff result saved to {lang}_{basedir}diff{a:03d}.{b-1:03d}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a8056d0-570d-4fac-b653-0f1368f3884f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T11:56:15.265755Z",
     "iopub.status.busy": "2024-05-07T11:56:15.265026Z",
     "iopub.status.idle": "2024-05-07T11:56:15.307131Z",
     "shell.execute_reply": "2024-05-07T11:56:15.305370Z",
     "shell.execute_reply.started": "2024-05-07T11:56:15.265691Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated results saved to es_res0515a/conso01.csv\n"
     ]
    }
   ],
   "source": [
    "# data via csv\n",
    "\n",
    "basedir = 'res0515a/'\n",
    "lang = 'es'\n",
    "url = f'https://{lang}.wikipedia.org/wiki/'\n",
    "data = []\n",
    "for i in range(0, 1000):\n",
    "    with open(f'{lang}_{basedir}links{i:03d}.csv') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    start = urlparse(df.iloc[0, 0]).path\n",
    "    dos = df.iloc[0, 1]\n",
    "    if not bool(urlparse(df.iloc[0, 0]).netloc):\n",
    "        path = df.iloc[:, 0].apply(lambda x: urljoin(url, x)).values.tolist()\n",
    "    else:\n",
    "        path = df.iloc[:, 0].values.tolist()\n",
    "    data.append([start, dos, path])\n",
    "n = 1\n",
    "df = pd.DataFrame(data, columns=['Start', 'DoS', 'Path'])\n",
    "df.to_csv(f'{lang}_{basedir}conso{n:02d}.csv', index=False)\n",
    "print(f'Consolidated results saved to {lang}_{basedir}conso{n:02d}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055031dc-4b24-48eb-bf1f-6b03b962d323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
